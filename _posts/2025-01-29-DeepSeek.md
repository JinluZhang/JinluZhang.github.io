---
title: 'DeepSeek学习笔记'
date: 2025-01-29
categories: 技术
author: Beaug
tags: 大模型 DeepSeek MoE
---

春节假期想干点儿轻松的，翻译+记笔记使我快乐。


# 一些HighLight

 - V2的关键是MLA和MoE；V3是；R1是

 - 【MLA】参考llama中的Attention模块详细拆解图。 https://jinluzhang.github.io/%E6%8A%80%E6%9C%AF/2023/08/02/LLAMA2-code.html 。比如我们在第T步的输入是500个token，输出了第501个token，那么继续自回归inference，在第T+1步要输入501个token，此时应该要计算这个501个token的WQ、WK、WV矩阵乘法、501个query和501个key之间的score，但前面的500个token的已经算过了，为了减少计算量，会把他们缓存下来。但这个key和value的缓存比较大，是2 * dim * token_length大小的，本文的MLA可以减少这部分的缓存空间。低秩是降低参数量的常用手段，LoRA里也有。

 - 【MLA】原本第一反应这是时间换空间，在MHA里，$W^K \mathbf{h}_t$ 和 $W^V \mathbf{h}_t$ 是不用计算的，结果已经被缓存了。但是MLA需要多算两次up-projection： $W^{UK} \mathbf{c}_t^{KV}$ 、 $W^{UV} \mathbf{c}_t^{KV}$  。 但巧妙在参数矩阵可以合并，并且合并后，对应的 $W^Q$ 和 $W^O$ size也降低了，和MHA相比inference时候的计算量也降低了。


---

# DeepSeek-V2

## Abstract

DeepSeek-V2是混合专家（Mixture-of-Experts, MoE）语言模型，其特点是经济的训练和高效的推理。该模型总共有 2360 亿个参数，其中每个 token 激活 210 亿个参数，并且支持 128K token 的上下文长度。DeepSeek-V2 采用了创新的架构，包括多头潜在注意力（Multi-head Latent Attention, MLA）和 DeepSeekMoE。MLA 通过显著压缩Key-Value缓存为latent向量，保证了高效的推理，而 DeepSeekMoE 则通过稀疏计算sparse computation，使得可以经济高效的训练出strong模型。与 DeepSeek 67B 相比，DeepSeek-V2 在性能上显著提升，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76 倍。我们在一个包含 8.1 万亿 token 的高质量多源语料库上对 DeepSeek-V2 进行了预训练，并进一步进行了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL），充分挖掘其潜力。评估结果显示，即使只有 210 亿激活参数，DeepSeek-V2 及其聊天版本在开源模型中仍然表现出色。

## 1.Introduction

在过去的几年中，大型语言模型（LLMs）（Anthropic, 2023；Google, 2023；OpenAI, 2022, 2023）经历了快速发展，为我们展现了人工通用智能（AGI）的曙光。通常，LLM 的智能随着参数数量的增加而提升，使其能够在各种任务中展示出涌现的能力（Wei et al., 2022）。然而，这种提升伴随着更大计算资源的需求，并可能导致推理吞吐量的下降。这些限制构成了显著的挑战，阻碍了 LLM 的广泛采用和利用。为了应对这一问题，我们提出了 DeepSeek-V2，一款强大的开源混合专家（Mixture-of-Experts, MoE）语言模型，采用创新的 Transformer 架构，使其训练经济、推理高效。该模型总共有 2360 亿参数，其中每个 token 激活 210 亿参数，并支持 128K token 的上下文长度。

我们优化了 Transformer 框架（Vaswani et al., 2017）中的Attention模块和前馈网络Feed-Forward Networks (FFNs)，引入了我们提出的多头潜在注意力（Multi-head Latent Attention, MLA）和 DeepSeekMoE。（1）在注意力机制的背景下，多头注意力（Multi-Head Attention, MHA）（Vaswani et al., 2017）中的键值（Key-Value, KV）缓存对 LLM 的推理效率构成了显著障碍。为了解决这个问题，已经探索了多种方法，包括分组查询注意力（Grouped-Query Attention, GQA）（Ainslie et al., 2023）和多查询注意力（Multi-Query Attention, MQA）（Shazeer, 2019）。然而，这些方法在试图减少 KV 缓存时，往往会牺牲性能。为了实现两者兼得，我们提出了 MLA，这是一种具备低秩键值联合压缩的注意力机制。从实验证据来看，MLA 相比 MHA 能够实现更优的性能，同时在推理过程中显著减少 KV 缓存，从而提升推理效率。（2）对于前馈网络（FFN），我们遵循 DeepSeekMoE 架构（Dai et al., 2024），该架构采用精细的专家分割和共享专家隔离，以提高专家专门化的潜力。与传统的 MoE 架构（如 GShard（Lepikhin et al., 2021））相比，DeepSeekMoE 架构展示了显著的优势，使我们能够以经济的成本训练强大的模型。由于我们在训练过程中采用了专家并行机制，我们还设计了补充机制来控制通信开销并确保负载均衡。通过结合这两种技术，DeepSeek-V2 同时具备了强大的性能、经济的训练成本和高效的推理吞吐量。

我们构建了一个高质量、多源的预训练语料库，包含 8.1 万亿 tokens。与 DeepSeek 67B（我们之前发布的版本）（DeepSeek-AI, 2024）所使用的语料库相比，这个语料库的数据量有所增加，尤其是中文数据，并且数据质量更高。我们首先在完整的预训练语料库上对 DeepSeek-V2 进行预训练。然后，我们收集了 150 万对话会话，涵盖了数学、代码、写作、推理、安全等多个领域，用于对 DeepSeek-V2 Chat 进行监督微调（Supervised Fine-Tuning, SFT）。最后，我们参照 DeepSeekMath（Shao et al., 2024）采用群体相对策略优化（Group Relative Policy Optimization, GRPO）进一步调整模型，以使其更符合人类偏好，并产生 DeepSeek-V2 Chat（RL）。

我们在英语和中文的一系列基准测试中评估了 DeepSeek-V2，并与代表性的开源模型进行了比较。评估结果显示，即使只有 210 亿个激活参数，DeepSeek-V2 仍然在开源模型中实现了顶级表现，成为最强的开源 MoE 语言模型。图 1(a) 显示，在 MMLU 测试中，DeepSeek-V2 在仅激活少量参数的情况下仍取得了领先的表现。此外，如图 1(b) 所示，与 DeepSeek 67B 相比，DeepSeek-V2 节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提升了 5.76 倍。我们还在开放式基准测试中评估了 DeepSeek-V2 Chat（SFT）和 DeepSeek-V2 Chat（RL）。值得注意的是，DeepSeek-V2 Chat（RL）在 AlpacaEval 2.0（Dubois et al., 2024）上的长度控制胜率为 38.9%，在 MT-Bench（Zheng et al., 2023）上的整体得分为 8.97，在 AlignBench（Liu et al., 2023）上的整体得分为 7.91。英语开放式对话评估表明，DeepSeek-V2 Chat（RL）在开源聊天模型中具有顶级表现。此外，在 AlignBench 上的评估结果表明，在中文环境下，DeepSeek-V2 Chat（RL）超过了所有开源模型，甚至超过了大多数闭源模型。

为了促进 MLA 和 DeepSeekMoE 的进一步研究与开发，我们还发布了 DeepSeek-V2-Lite，这是一个较小的模型，配备了 MLA 和 DeepSeekMoE，供开源社区使用。该模型总共有 157 亿个参数，每个 token 激活 24 亿个参数。关于 DeepSeek-V2-Lite 的详细描述可以在附录 B 中找到。

本文的其余部分将首先详细描述 DeepSeek-V2 的模型架构（第 2 节）。随后，我们将介绍我们的预训练工作，包括训练数据的构建、超参数设置、基础设施、长上下文扩展以及模型性能和效率的评估（第 3 节）。接下来，我们将展示我们在对齐方面的努力，包括监督微调（SFT）、强化学习（RL）、评估结果和其他讨论（第 4 节）。最后，我们总结结论，讨论 DeepSeek-V2 的当前局限性，并概述我们的未来工作（第 5 节）。



## 2.Architecture

总体来说，DeepSeek-V2 仍然采用了 Transformer 架构（Vaswani 等人，2017），其中每个 Transformer 模块由一个注意力模块和一个前馈网络（FFN）组成。然而，针对注意力模块和 FFN，我们设计并采用了创新的架构。对于注意力，我们设计了 MLA，它利用低秩的键值联合压缩来消除推理时键值缓存的瓶颈，从而支持高效的推理。对于 FFN，我们采用了 DeepSeekMoE 架构（Dai 等人，2024），这是一种高性能的 MoE 架构，能够以经济的成本训练出强大的模型。DeepSeek-V2 的架构示意图如图所示，我们将在本节中介绍 MLA 和 DeepSeekMoE 的详细信息。至于其他一些细节（例如层归一化和 FFN 中的激活函数），除非特别说明，DeepSeek-V2 遵循 DeepSeek 67B（DeepSeek-AI，2024）的设置。

![图1](http://jinluzhang.github.io/assets/posts_img/2025-01-29-DeepSeek/deepseek-v2-1.png)

## 2.1.Multi-Head Latent Attention

传统的 Transformer 模型通常采用多头注意力（MHA）（Vaswani 等人，2017），但在生成过程中，其庞大的键值缓存会成为限制推理效率的瓶颈。为了减少 KV 缓存，提出了多查询注意力（MQA）（Shazeer，2019）和分组查询注意力（GQA）（Ainslie 等人，2023）。这些方法需要更小的 KV 缓存，但它们的性能无法与 MHA 匹敌（我们在附录 D.1 中提供了 MHA、GQA 和 MQA 的消融实验）。

对于 DeepSeek-V2，我们设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。MLA 通过低秩键值联合压缩，实现了比 MHA 更好的性能，同时需要显著更少的 KV 缓存。我们将在以下部分介绍其架构，并在附录 D.2 中提供 MLA 与 MHA 的对比。

![图1](http://jinluzhang.github.io/assets/posts_img/2025-01-29-DeepSeek/deepseek-v2-2.png)


### 2.1.1 标准的多头注意力

我们首先介绍标准的 MHA 机制作为背景。记 $d$ 表示嵌入维度（embedding dimension），$n_h$ 表示注意力头的数量，$d_h$ 表示每个注意力头的维度，$\mathbf{h}_t \in \mathbb{R}^d$ 表示给定的注意力层中第t个token对应的attention输入。标准 MHA 首先通过三个矩阵 $W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}$ 分别生成 $\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t \in \mathbb{R}^{d_h n_h}$ :

$$
\mathbf{q}_t = W^Q \mathbf{h}_t, \tag{1}
$$

$$
\mathbf{k}_t = W^K \mathbf{h}_t, \tag{2}
$$

$$
\mathbf{v}_t = W^V \mathbf{h}_t, \tag{3}
$$

然后，$\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t$ 会切成 $n_h$ 块，分别送入 $n_h$ 个多个头做计算。

$$
[\mathbf{q}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{q}_{t,n_h}] = \mathbf{q}_t,
$$

$$
[\mathbf{k}_{t,1}; \mathbf{k}_{t,2}; \dots; \mathbf{k}_{t,n_h}] = \mathbf{k}_t,
$$

$$
[\mathbf{v}_{t,1}; \mathbf{v}_{t,2}; \dots; \mathbf{v}_{t,n_h}] = \mathbf{v}_t, \tag{4}
$$

以第i个Attention头为例，记它第t个token对应的的k向量 $\mathbf{k}^i$ 如下，v类似。

$$
\mathbf{k}^i = [\mathbf{k}_{1,i}; \mathbf{k}_{2,i}; \dots; \mathbf{k}_{t,i}],
$$

$$
\mathbf{v}^i = [\mathbf{v}_{1,i}; \mathbf{v}_{2,i}; \dots; \mathbf{v}_{t,i}], \tag{5}
$$


然后, $\mathbf{q}_{t,i}$ 要和所有keys计算MatMul，除以 $\sqrt{d_h}$ 后计算softmax，代表对所有values的权重。记 $\mathbf{q}_{t,i}$ 对应的输出是 $\mathbf{o}_{t,i}$ :

$$
 \mathbf{o}_{t,i} = \text{Softmax}\bigl(\frac{\mathbf{q}_{t,i} \mathbf{k}^i}{\sqrt{d_h}}\bigr) \mathbf{v}^i, \tag{6}
$$

然后，所有 $\mathbf{o}_{t,i}$ concat在一起后乘以$W^O \in \mathbb{R}^{d \times d_h n_h}$ ，就是第t个token对应的MHA的整体输出：

$$
\mathbf{u}_t = W^O [\mathbf{o}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{q}_{t,n_h}], \tag{7}
$$

在推理过程中，所有的 $\mathbf{k_t}, \mathbf{v_t}$ 需要缓存以加速推理，因此在推理时，每层需要缓存 2×𝑑×sequence_length 个元素。在模型部署时，这种庞大的 KV 缓存成为了限制maximum batch和sequence length的瓶颈。

<span style="color:red">参考llama中的Attention模块详细拆解图。 https://jinluzhang.github.io/%E6%8A%80%E6%9C%AF/2023/08/02/LLAMA2-code.html 。比如我们在第T步的输入是500个token，输出了第501个token，那么继续自回归inference，在第T+1步要输入501个token，此时应该要计算这个501个token的WQ、WK、WV矩阵乘法、501个key和501个value之间的score，但前面的500个token的已经算过了，为了减少计算量，会把他们缓存下来。但这个缓存比较大，是dim X token_length大小的，本文的MLA可以减少这部分的缓存空间。注意在第T+1步中，我们的输出是第502个token，query是已知的501个token，但在这一步，只有第501个token的score是没计算过的，所以在这一步中的query一般就只记做第501个token它自己。以及，按逻辑每一层的output也要缓存的。但因为kv缓存在，下一层真正用到的未知的就只有第501个token对应的那个输出，所以output就不需要缓存了</span>

### 2.1.2 Low-Rank Key-Value Joint Compression

MLA 的核心是对键和值进行低秩联合压缩low-rank joint compression以减少 KV 缓存：

$$
\mathbf{c}_t^{KV} = W^{DKV} \mathbf{h}_t, \tag{8}
$$

$$
\mathbf{k}_t^C = W^{UK} \mathbf{c}_t^{KV}, \tag{9}
$$

$$
\mathbf{v}_t^C = W^{UV} \mathbf{c}_t^{KV}, \tag{10}
$$
 
其中， $\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}$ 是压缩后的latent向量， $W^{DKV} \in \mathbb{R}^{d_c \times d}$ 为down-projection下投影矩阵； $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别为key和value的上投影矩阵。

在推理过程中，MLA 仅需要缓存 $\mathbf{c}_t^{KV}$ ，因此其 KV 缓存只有 $d_c$ ×sequence_length个元素。
此外，在推理时，由于 $W^{UK}$ 可以合并进 $W^Q$ 中，$W^{UV}$ 可以合并进 $W^O$ 中，因此我们甚至无需付出额外更多的计算。

<span style="color:red">原本第一反应这是时间换空间，在MHA里，$W^K \mathbf{h}_t$ 和 $W^V \mathbf{h}_t$ 是不用计算的，结果已经被缓存了。但是MLA需要多算两次up-projection： $W^{UK} \mathbf{c}_t^{KV}$ 、 $W^{UV} \mathbf{c}_t^{KV}$  。 但巧妙在参数矩阵可以合并，并且合并后，对应的 $W^Q$ 和 $W^O$ size也降低了，和MHA相比inference时候的计算量也降低了。（训练的时候没有做这个合并，所以逻辑上参数空间没有被压缩，模型的表达能力没有被压缩）。</span>

为了减少训练过程中的激活内存，我们还对query进行了低秩压缩（就是对 $W^Q$ 做了低秩压缩分解，实际上参数量变少了，模型表达能力按道理也降低了），尽管这不能减少 KV 缓存：

$$
\mathbf{c}_t^Q = W^{DQ} \mathbf{h}_t, \tag{11}
$$

$$
\mathbf{q}_t^C = W^{UQ} \mathbf{c}_t^Q, \tag{12}
$$


### 2.1.3. Decoupled Rotary Position Embedding

延续 DeepSeek 67B（DeepSeek-AI，2024），我们想在 DeepSeek-V2 中也使用旋转位置嵌入（Rotary Position Embedding，RoPE）（Su 等人，2024）。然而，RoPE 与低秩键值压缩不兼容。具体而言，RoPE 对于keys和queries都是位置敏感的。如果我们将 RoPE 应用于 $\mathbf{k}_t^C$ ，那么公式（9）中的 $W^{UK}$ 将与一个位置敏感的 RoPE 矩阵耦合。这样，在推理过程中，它就无法再被合并到 $W^Q$ 中，因为与当前正在生成的token相关的 RoPE 矩阵位于 $W^Q$ 和 $W^{UK}$ 之间，而矩阵乘法并不满足交换律。因此，我们必须在推理过程中重新计算所有前序token的keys，这将显著影响推理效率。

为了解决这个问题，我们提出了一种解耦 RoPE 策略，该策略使用额外的多头queries  $\mathbf{q}_{t,i}^R \in \mathbb{R}^{d_h^R}$ 和一个共享key  $\mathbf{k}_t^R \in \mathbb{R}^{d_h^R}$ 来携带RoPE信息，其中 $d_h^R$ 代表每个头中，解耦了的queries和key的维度大小。通过 RoPE 解耦策略，MLA 执行以下计算：

$$
 [ \mathbf{q}_{t,1}^R; \mathbf{q}_{t,2}^R; \dots; \mathbf{q}_{t,n_h}^R] = \mathbf{q}_t^R = \text{RoPE}(W^{QR} \mathbf{c}_t^Q) = \text{RoPE}(W^{QR} W^{DQ} \mathbf{h}_t), \tag{13}
$$

$$
\mathbf{k}_t^R = \text{RoPE}(W^{KR} \mathbf{h}_t), \tag{14}
$$

$$
\mathbf{q}_{t,i} = [\mathbf{q}_{t,i}^C; \mathbf{q}_{t,i}^R],
$$

$$
\mathbf{k}_{t,i} = [\mathbf{k}_{t,i}^C; \mathbf{k}_{t}^R], \tag{14}
$$



其中，`W_QR ∈ R^{d_R * n_h} × d'` 和 `W_KR ∈ R^{d_R} × d` 分别是生成解耦查询和键的矩阵；`RoPE(·)` 表示应用 RoPE 矩阵的操作；`[·; ·]` 表示连接操作。在推理过程中，解耦的键也需要缓存。因此，DeepSeek-V2 需要一个总共包含 `(d + d_R)l` 个元素的 KV 缓存。


### 2.1.4 键值缓存的比较
我们在表 1 中展示了不同注意力机制每个令牌的 KV 缓存比较。MLA 只需要少量的 KV 缓存，等于 GQA 只有 2.25 个组，但能够实现比 MHA 更强的性能。

**表 1** | 不同注意力机制每个令牌的 KV 缓存比较。`n_h` 表示注意力头的数量，`d_h` 表示每个注意力头的维度，`l` 表示层数，`n` 表示 GQA 中的组数，`d` 和 `d_R` 分别表示 KV 压缩维度和 MLA 中解耦查询和键的每头维度。KV 缓存的量通过元素数量来度量，不考虑存储精度。对于 DeepSeek-V2，`d_c` 设置为 `4d_h`，`d_R` 设置为 `d_h^2`。因此，其 KV 缓存等于 GQA 只有 2.25 个组，但其性能强于 MHA。







## 2.2. DeepSeekMoE: Training Strong Models at Economical Costs






## 例子-DeepSeek-V2-Lite

以DeepSeek-V2-Lite模型为例，模型定义：https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py

关键参数如下。

模型Total Params=16B，Activated Params=2.4B，Context Length=32k

```python

        vocab_size=102400,
        hidden_size=4096,
        intermediate_size=11008,
        moe_intermediate_size = 1407,
        num_hidden_layers=30,
        num_attention_heads=32,
        num_key_value_heads=32,
        n_shared_experts = None,
        n_routed_experts = None,
        ep_size = 1,
        routed_scaling_factor = 1.0,
        kv_lora_rank = 512,
        q_lora_rank = 1536,
        qk_rope_head_dim = 64,
        v_head_dim = 128,
        qk_nope_head_dim = 128,
        topk_method = 'gready',
        n_group = None,
        topk_group = None,
        num_experts_per_tok = None,
        moe_layer_freq = 1,
        first_k_dense_replace = 0,
        norm_topk_prob = False,
        scoring_func = 'softmax',
        aux_loss_alpha = 0.001,
        seq_aux = True,
        hidden_act="silu",
        max_position_embeddings=2048,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=None,
        bos_token_id=100000,
        eos_token_id=100001,
        pretraining_tp=1,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        rope_scaling=None,
        attention_bias=False,
        attention_dropout=0.0

{
  "architectures": [
    "DeepseekV2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV2Config",
    "AutoModel": "modeling_deepseek.DeepseekV2Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV2ForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "first_k_dense_replace": 1,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 10944,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "model_type": "deepseek_v2",
  "moe_intermediate_size": 1408,
  "moe_layer_freq": 1,
  "n_group": 1,
  "n_routed_experts": 64,
  "n_shared_experts": 2,
  "norm_topk_prob": false,
  "num_attention_heads": 16,
  "num_experts_per_tok": 6,
  "num_hidden_layers": 27,
  "num_key_value_heads": 16,
  "pretraining_tp": 1,
  "q_lora_rank": null,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 0.707,
    "mscale_all_dim": 0.707,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 1.0,
  "scoring_func": "softmax",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "topk_group": 1,
  "topk_method": "greedy",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.33.1",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 102400
}
```

模型结构及每一个参数矩阵的具体size如下。layer.1到layer.26结构一样；每个layer里，experts.0到experts.63结构一样。

```python
{
  "metadata": {
    "total_size": 31412968448
  },
  "weight_map": {
    "model.embed_tokens.weight": {"dtype":"BF16","shape":[102400,2048]},
    "model.norm.weight": {"dtype":"BF16","shape":[2048]},
    "lm_head.weight": {"dtype":"BF16","shape":[102400,2048]},

    "model.layers.0.self_attn.q_proj.weight": {"dtype":"BF16","shape":[3072,2048]},
    "model.layers.0.self_attn.kv_a_proj_with_mqa.weight": {"dtype":"BF16","shape":[576,2048]},
    "model.layers.0.self_attn.kv_a_layernorm.weight": {"dtype":"BF16","shape":[512]},
    "model.layers.0.self_attn.kv_b_proj.weight": {"dtype":"BF16","shape":[4096,512]},
    "model.layers.0.self_attn.o_proj.weight": {"dtype":"BF16","shape":[2048,2048]},
    "model.layers.0.mlp.gate_proj.weight": {"dtype":"BF16","shape":[10944,2048]},
    "model.layers.0.mlp.up_proj.weight": {"dtype":"BF16","shape":[10944,2048]},
    "model.layers.0.mlp.down_proj.weight": {"dtype":"BF16","shape":[2048,10944]},
    "model.layers.0.input_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    "model.layers.0.post_attention_layernorm.weight": {"dtype":"BF16","shape":[2048],
    "model.layers.1.self_attn.q_proj.weight": {"dtype":"BF16","shape":[3072,2048]},
    "model.layers.1.self_attn.kv_a_proj_with_mqa.weight": {"dtype":"BF16","shape":[576,2048]},
    "model.layers.1.self_attn.kv_a_layernorm.weight": {"dtype":"BF16","shape":[512]},
    "model.layers.1.self_attn.kv_b_proj.weight": {"dtype":"BF16","shape":[4096,512]},
    "model.layers.1.self_attn.o_proj.weight": {"dtype":"BF16","shape":[2048,2048]},
    "model.layers.1.mlp.gate.weight": {"dtype":"BF16","shape":[64,2048]},
    "model.layers.1.mlp.shared_experts.gate_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.1.mlp.shared_experts.up_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.1.mlp.shared_experts.down_proj.weight": {"dtype":"BF16","shape":[2048,2816]},
    "model.layers.1.mlp.experts.0.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.0.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.0.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
    "model.layers.1.mlp.experts.1.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.1.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.1.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
     ...
    "model.layers.1.mlp.experts.63.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.63.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.63.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
    "model.layers.1.input_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    "model.layers.1.post_attention_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    ...
    "model.layers.26.mlp.experts.63.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.26.mlp.experts.63.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.26.mlp.experts.63.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},

```


---

# DeepSeek-V3

## Abstract

DeepSeek-V3是混合专家（MoE）语言模型，总参数量6710亿，其中每个token激活参数量为370亿。为实现高效推理与经济训练，DeepSeek-V3沿用了在DeepSeek-V2中经过充分验证的多头潜在注意力（MLA Multi-head Latent Attention）架构与DeepSeekMoE架构，并开创性地引入无辅助损失的负载均衡策略（an auxiliary-loss-free strategy for load balancing），同时采用multi-token prediction训练目标以提升模型性能。我们基于14.8万亿高质量多源语料对DeepSeek-V3进行预训练，并通过监督微调与强化学习阶段充分释放其能力。综合评估表明，DeepSeek-V3不仅显著超越其他开源模型，性能更可媲美顶尖闭源模型。该模型的完整训练仅需278.8万H800 GPU小时，且训练过程异常稳定，全程未发生任何不可恢复的损失尖峰，也未进行任何回滚操作。
模型检查点已发布于：https://github.com/deepseek-ai/DeepSeek-V3

## 1.Introduction

近年来，大型语言模型（LLM）经历了快速迭代与进化（Anthropic, 2024；Google, 2024；OpenAI, 2024a），不断逼近通用人工智能（AGI）的边界。除闭源模型外，以DeepSeek系列（DeepSeek-AI, 2024a,b,c；Guo等，2024）、LLaMA系列（AI@Meta, 2024a,b；Touvron等，2023a,b）、Qwen系列（Qwen, 2023, 2024a,b）及Mistral系列（Jiang等，2023；Mistral, 2024）为代表的开源模型也取得显著进展，持续缩小与闭源模型的差距。为进一步突破开源模型能力边界，我们通过scale up 推出DeepSeek-V3——总参数量达6710亿的混合专家（MoE）模型，其中每个token激活参数量为370亿。

基于前瞻性视角，我们始终追求卓越性能与经济成本的平衡。因此，在架构层面，为了实现高效推理，沿用多头潜在注意力（MLA）（DeepSeek-AI, 2024c），为了高效训练，仍沿用DeepSeekMoE架构（Dai等，2024）。这两项技术经过了DeepSeek-V2验证。在此基础上，我们引入两项创新策略：首先，DeepSeek-V3 引入了一种不依赖辅助损失（auxiliary-loss-free）的负载均衡策略（Wang 等, 2024a），旨在最大程度地降低为实现负载均衡而对模型性能产生的负面影响。其次，DeepSeek-V3 采用了多 token 预测的训练目标，我们观察到这一策略有助于提升在各项评测基准上的整体表现。

通过FP8混合精度训练框架实现训练加速与显存优化。低精度训练作为高效训练的有效方案（Dettmers等，2022；Kalamkar等，2019；Narang等，2017；Peng等，2023b），其发展与硬件能力演进密切关联（Luo等，2024；Micikevicius等，2022；Rouhani等，2023a）。本研究首次在超大规模模型上验证FP8训练的有效性，通过FP8计算与存储支持实现训练速度提升与显存占用降低。

为了实现高效训练，我们支持 FP8 混合精度训练，并对训练框架进行全面优化。低精度训练已经成为提升训练效率的有力解决方案（Dettmers 等, 2022；Kalamkar 等, 2019；Narang 等, 2017；Peng 等, 2023b），其发展与硬件能力的进步密切相关（Luo 等, 2024；Micikevicius 等, 2022；Rouhani 等, 2023a）。在本研究中，我们首次在超大规模模型上引入并验证了 FP8 混合精度训练框架的有效性。通过支持 FP8 的计算与存储，我们不仅加速了训练速度，还减少了 GPU 内存的占用。在训练框架方面，我们设计了 DualPipe 算法以实现高效的流水线并行。该算法减少了pipeline bubbles，并通过计算与通信的重叠（computation-communication overlap）在训练过程中隐藏了大部分通信开销。这种方式可以保证在模型规模进一步扩张时，只要维持恒定的计算与通信比率，仍能在跨节点间使用细粒度专家fine-grained experts，并将 all-to-all 通信的开销降至近乎为零。此外，我们还开发了高效的跨节点 all-to-all 通信内核，以充分利用 InfiniBand (IB) 与 NVLink 的带宽。我们也对内存占用进行了精细化优化，使得在不依赖高成本张量并行的情况下，依然可以训练 DeepSeek-V3。通过这些努力，我们实现了高效的训练过程。

在预训练阶段，我们使用 14.8T 高质量且多样化的 token 对 DeepSeek-V3 进行训练，整个预训练过程十分稳定。期间没有出现无法恢复的损失爆增，也无需回退。接下来，我们对 DeepSeek-V3 进行了两阶段的上下文长度扩展：第一阶段将最大上下文长度扩展至 32K，第二阶段进一步扩展至 128K。随后，我们对 DeepSeek-V3 的基础模型进行后训练（post-training），包括有监督微调（SFT）和强化学习（RL），以使其更贴合人类偏好并进一步释放模型潜力。在后训练阶段，我们从 DeepSeek-R1 系列模型中蒸馏其推理能力，并在此过程中谨慎地平衡模型的准确性与生成长度。

我们在大量基准上对 DeepSeek-V3 进行了全面评估。虽然训练成本较为经济，但综合评测结果显示，DeepSeek-V3-Base 已成为目前最强的开源基础模型，尤其在代码与数学方面表现突出。其聊天版本在多项标准和开放式基准上也优于其他开源模型，并在性能上可比肩主流闭源模型（包括 GPT-4o 和 Claude-3.5-Sonnet）。

最后，我们再次强调 DeepSeek-V3 所具有的经济训练成本（见表 1），这是通过算法、框架与硬件的协同优化共同实现的。在预训练阶段，DeepSeek-V3 每训练 1 万亿个 token 仅需消耗 18 万个 H800 GPU 小时；换言之，在拥有 2048 张 H800 GPU 的集群上只需运行 3.7 天即可完成。这使得整个预训练阶段在不到两个月的时间内完成，耗费了共 266.4 万 GPU 小时。再加上上下文长度扩展所用的 11.9 万 GPU 小时以及后训练所需的 0.5 万 GPU 小时，DeepSeek-V3 的完整训练总耗时仅为约 278.8 万 GPU 小时。若按每 GPU 小时 2 美元的租用价格计算，总训练成本仅为 557.6 万美元。需要注意的是，上述成本仅包括 DeepSeek-V3 的正式训练，不包含在模型架构、算法或数据等方面进行前期研究和消融实验的费用。

核心创新贡献如下：

**Architecture: Innovative Load Balancing Strategy and Training Objective**

 - 在 DeepSeek-V2 高效架构的基础上，我们率先提出了一种不依赖辅助损失的负载均衡策略，最大限度地减少了为实现负载均衡而导致的模型性能损失。

 - 我们探索了Multi-Token Prediction（MTP）目标，并证明它可以提高模型性能。同时，该目标也可用于推理加速中的推测解码（speculative decoding）。

**Pre-Training: Towards Ultimate Training Efficiency**

 - 我们设计了一套 FP8 混合精度训练框架，并首次在超大规模模型上验证了 FP8 训练的可行性与有效性。

 - 通过算法、框架与硬件的协同设计，我们突破了跨节点 MoE 训练中的通信瓶颈，实现了几乎完全的计算与通信重叠。此举不仅大幅提升了训练效率并降低了训练成本，也使得我们可以在不增加额外开销的情况下进一步扩大模型规模。

 - 仅用 266.4 万个 H800 GPU 小时的经济成本，我们完成了 DeepSeek-V3 在 14.8 万亿条文本上的预训练，打造了目前最强的开源基础模型。随后的后续训练阶段仅需 10 万个 GPU 小时。

**Post-Training: Knowledge Distillation from DeepSeek-R1**

 - 我们提出了一种创新方法，将CoT模型（Chain-of-Thought,DeepSeek-R1 系列模型之一）中的推理能力蒸馏到标准大语言模型中，尤其是 DeepSeek-V3。该流程将 R1 模型的验证与反思模式优雅地融合至 DeepSeek-V3，显著提升了其推理性能。同时，我们也对 DeepSeek-V3 的输出风格与长度加以控制。

**核心评测结果概览**

 - 知识：在 MMLU、MMLU-Pro 与 GPQA 等教育类基准上，DeepSeek-V3 超越了所有其他开源模型，分别取得了 88.5、75.9 和 59.1 的成绩，其表现可与 GPT-4o 和 Claude-Sonnet-3.5 等领先闭源模型相媲美，进一步缩小了开源与闭源模型在此领域的差距。
在客观事实相关基准上，DeepSeek-V3 在 SimpleQA 与 Chinese SimpleQA 两个数据集中均优于其他开源模型。尽管在英文事实类知识（SimpleQA）方面仍略逊于 GPT-4o 与 Claude-Sonnet-3.5，但在中文事实类知识（Chinese SimpleQA）上却超过了这些模型，展现出其在中文事实类知识方面的优势。

 - 代码、数学与推理：在数学相关基准上，DeepSeek-V3 在所有non-long-CoT的开源与闭源模型中取得了最先进的表现。值得注意的是，针对部分基准（如 MATH-500），其表现甚至超过了 o1-preview，展现了强大的数学推理能力。在编程相关任务上，DeepSeek-V3 在编程竞赛基准（如 LiveCodeBench）中取得最佳表现，牢牢占据该领域的领先地位。对于工程类任务，虽然 DeepSeek-V3 略低于 Claude-Sonnet-3.5，但依然远超其他所有模型，体现了其在多样化技术基准上的竞争力。

在后续的内容中，我们将首先详细介绍 DeepSeek-V3 的模型架构（第 2 节），包括高效推理所用的 Multi-head Latent Attention (MLA)（DeepSeek-AI, 2024c）以及经济训练所用的 DeepSeekMoE（Dai 等, 2024）等。随后，我们介绍所使用的基础设施，包括计算集群、训练框架、对 FP8 训练的支持、推理部署策略及未来硬件设计建议。接着，我们阐述预训练流程，包括训练数据的构建、超参数设置、长上下文扩展技术及相关评测与讨论（第 4 节）。然后，我们讨论后训练工作，包括有监督微调（SFT）、强化学习（RL）以及相应的评测与讨论（第 5 节）。最后，我们总结本项研究的结论，探讨 DeepSeek-V3 目前存在的限制并展望未来可能的研究方向（第 6 节）。



## 2.Architecture

我们首先介绍 DeepSeek-V3 的基本架构，其中采用了 Multi-head Latent Attention (MLA)（DeepSeek-AI, 2024c）来实现高效推理，并使用 DeepSeekMoE（Dai 等, 2024）来降低训练成本。随后，我们将介绍多 Token 预测（MTP）训练目标，该目标在评测基准上能够提升模型整体性能。除非另有说明，DeepSeek-V3 的其他细节均与 DeepSeek-V2（DeepSeek-AI, 2024c）中的设置保持一致。

### 2.1 基本架构

DeepSeek-V3 的整体框架仍然基于 Transformer（Vaswani 等, 2017）。为了实现高效推理与经济可控的训练成本，DeepSeek-V3 继续沿用 MLA 和 DeepSeekMoE，这两种方法在 DeepSeek-V2 中已得到充分验证。与 DeepSeek-V2 相比，主要的区别在于，我们在 DeepSeekMoE 中额外引入了一种无辅助损失（auxiliary-loss-free）的负载均衡策略（Wang 等, 2024a），以减轻因实现负载均衡而导致的性能下降。图 2 展示了 DeepSeek-V3 的基本架构，本节我们将简要回顾 MLA 与 DeepSeekMoE 的相关细节。

#### 2.1.1 Multi-Head Latent Attention


在注意力（attention）机制上，DeepSeek-V3 采用 MLA 架构。记 $d$ 表示嵌入维度（embedding dimension），$n_h$ 表示注意力头的数量，$d_h$ 表示每个注意力头的维度， $\mathbf{h}_t \in \mathbb{R}^d$ 表示给定的注意力层中第t个token对应的attention输入。 <span style="color:red">【self attention，q k v都是这个h】</span> MLA的核心在于对key和value的低秩联合压缩low-rank joint compression，从而在推理阶段减少Key-Value cache。

$$
\boxed{\textcolor{blue}{\mathbf{c}_{t}^{KV}}} = W^{DKV} \mathbf{h}_t \tag{1}
$$

$$
[\mathbf{k}_{t,1}^C; \mathbf{k}_{t,2}^C; \dots; \mathbf{k}_{t,n_h}^C] = \mathbf{k}_t^C = W^{UK} \mathbf{c}_{t}^{KV} \tag{2}
$$

$$
\boxed{\textcolor{blue}{\mathbf{k}_t^R}} = \text{RoPE}(W^{KR} \mathbf{h}_t) \tag{3}
$$

$$
\mathbf{k}_{t,i} = [\mathbf{k}_{t,i}^C; \mathbf{k}_{t}^R] \tag{4}
$$

$$
[\mathbf{v}_{t,1}^C; \mathbf{v}_{t,2}^C; \dots; \mathbf{v}_{t,n_h}^C] = \mathbf{v}_t^C = W^{UV} \mathbf{c}_{t}^{KV} \tag{5}
$$

$\mathbf{c}_{t}^{KV} \in \mathbb{R}^{d_c}$ 是keys和values经过压缩的latent向量， $d_c$  为压缩维度； $W^{DKV} \in \mathbb{R}^{d_c \times d}$ 为向下投影矩阵（down-projection）， $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别是键和值的向上投影矩阵（up-projection）； $W^{KR} \in \mathbb{R}^{d_h^R \times d}\) 乘以原始输入计算旋转位置编码的key h后会送入RoPE函数，使其携带Rotary Positional Embedding (RoPE）； $\mathrm{RoPE}(\cdot)$ 表示将 RoPE 矩阵应用于输入向量，[·;·] 表示向量拼接。需要注意的是，对于 MLA，在推理时只需缓存蓝色框所示的向量（即 $\textcolor{blue}{\mathbf{c}_{t}^{KV}}$ 与 $\textcolor{blue}{\mathbf{k}_t^R}$），这大幅减少了 KV 缓存的大小，同时性能仍能与标准多头注意力（MHA, Vaswani 等, 2017）相当。

对于注意力查询（query）也进行低秩压缩，从而在训练阶段减小激活值（activation）所占的内存：  

$$
c_t^Q,t = W^{DQ} \mathbf{h}_t, \tag{6}
$$

$$
[\mathbf{q}_{t,1}^C; q_{t,2}^C; \dots; q_{t,n_h}^C] = q^C_t = W^{UQ} c_t^Q}, \tag{7}
$$

$$
[q^R_{t,1}; q^R_{t,2}; \dots; q^R_{t,n_h}]
=
q^R_t
=
\mathrm{RoPE}\bigl(W_{Q\!R} \, c_{Q,t}\bigr),
\tag{8}
$$

由此可得  
\[
q_{t,i} = [q^C_{t,i}; \, q^R_{t}],
\tag{9}
\]  
其中 \(c_{Q,t} \in \mathbb{R}^{d'_c}\) 表示查询向量的压缩潜在表示，\(d'_c(\ll d_h \cdot n_h)\) 为查询的压缩维度；\(W_{D\!Q} \in \mathbb{R}^{d'_c \times d}\) 与 \(W_{U\!Q} \in \mathbb{R}^{d_h n_h \times d'_c}\) 分别是对查询向量的向下和向上投影矩阵；\(W_{Q\!R} \in \mathbb{R}^{d_R n_h \times d'_c}\) 用于生成携带 RoPE 的“解耦”查询向量。

最终，注意力的输出向量 \(u_t\) 由查询 \(q_{t,i}\)、键 \(k_{t,i}\) 与值 \(v^C_{t,i}\) 共同计算得到：  
\[
o_{t,i} = \sum_{j=1}^{\,}
\mathrm{Softmax}_j\!
\bigl(\frac{q_{t,i}^T\,k_{j,i}}{\sqrt{d_h + d_R}}\bigr)
\, v^C_{j,i},
\tag{10}
\]  
\[
u_t = W_{O} \,[o_{t,1}; o_{t,2}; \dots; o_{t,n_h}],
\tag{11}
\]  
其中 \(W_{O} \in \mathbb{R}^{d \times (d_h n_h)}\) 是输出投影矩阵。

以上便是 MLA 的主要细节。通过这种低秩压缩策略，我们既能在推理阶段大幅减少键值缓存的占用，又能在训练时有效降低内存消耗，从而在保持高性能的同时实现高效率与低成本。


高斯积分：

$$
\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}
$$

矩阵乘法：

$$
\begin{bmatrix}
1 & 2 \\
3 & 4 
\end{bmatrix}
\times
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix}
=
\begin{bmatrix}
1a+2c & 1b+2d \\
3a+4c & 3b+4d 
\end{bmatrix}
$$



# 参考文献


