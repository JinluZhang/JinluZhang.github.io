---
title: 'DeepSeek学习笔记'
date: 2025-01-29
categories: 技术
author: Beaug
tags: 大模型 DeepSeekMoE
---

春节假期想干点儿轻松的，翻译+记笔记使我快乐。


# 一些HighLight

 - 整体创新点为：DeepSeekMoE、MLA（于V2提出）；V3；R1

 - 【MLA】参考llama中的Attention模块详细拆解图。 https://jinluzhang.github.io/%E6%8A%80%E6%9C%AF/2023/08/02/LLAMA2-code.html 。比如我们在第T步的输入是500个token，输出了第501个token，那么继续自回归inference，在第T+1步要输入501个token，此时应该要计算这个501个token的WQ、WK、WV矩阵乘法、501个query和501个key之间的score，但前面的500个token的已经算过了，为了减少计算量，会把他们缓存下来。但这个key和value的缓存比较大，是2 * dim * token_length大小的，本文的MLA可以减少这部分的缓存空间。低秩是降低参数量的常用手段，LoRA里也有。

 - 【MLA】原本第一反应这是时间换空间，在MHA里，$W^K \mathbf{h}_t$ 和 $W^V \mathbf{h}_t$ 是不用计算的，结果已经被缓存了。但是MLA需要多算两次up-projection： $W^{UK} \mathbf{c}_t^{KV}$ 、 $W^{UV} \mathbf{c}_t^{KV}$  。 但巧妙在参数矩阵可以合并，并且合并后，对应的 $W^Q$ 和 $W^O$ size也降低了，和MHA相比inference时候的计算量也降低了。


---

# DeepSeekMoE

## Abstract

在大规模语言模型的时代，混合专家（MoE）是一种有前景的架构，能够在扩展模型参数时管理计算成本。然而，传统的MoE架构，如GShard，它通过激活从 N 个专家中选取的 top-K 个专家，面临着确保专家专长的挑战（即每个专家获取不重叠且集中的知识）。为此，我们提出了DeepSeekMoE架构，旨在实现终极专家专长 ultimate expert specialization。它包含两个主要策略：（1）将专家细分为更细粒度的 $mN$ 个，并激活其中的 $mK$ 个，使得有更灵活的激专家组合；（2）隔离出 $K_s$ 个专家为共享专家，旨在捕捉共同的知识并减少路由专家之间的冗余。

我们从2B参数的规模开始，展示了DeepSeekMoE 2B在性能上与GShard 2.9B相当，后者有1.5倍的专家参数和计算量。此外，DeepSeekMoE 2B几乎达到了与其密集版本相同参数数量的性能，后者设定了MoE模型的上限。随后，我们将DeepSeekMoE扩展到16B参数，展示其在计算量仅约为40%的情况下，达到了与LLaMA2 7B相当的性能。进一步地，我们初步将DeepSeekMoE扩展到145B参数，验证了它相较于GShard架构的显著优势，并显示出与DeepSeek 67B相当的性能，仅使用了28.5%（甚至可能是18.2%）的计算量。

## 1. Introduction

近年来的研究和实践已实验证明，若有足够的训练数据，通过增加参数和计算预算扩展语言模型，可以获得显著更强的模型（Brown et al., 2020; Hoffmann et al., 2022; OpenAI, 2023; Touvron et al., 2023a）。然而，需要注意的是，扩展模型至极大规模也伴随着极高的计算成本。考虑到这一巨大的成本，混合专家（MoE）架构（Jacobs et al., 1991; Jordan 和 Jacobs, 1994; Shazeer et al., 2017）成为了一种受欢迎的解决方案。它能够支持参数扩展，同时保持计算成本在合理水平。MoE架构在Transformers中的应用（Vaswani et al., 2017）已成功地尝试了将语言模型扩展到相当大的规模（Du et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021; Zoph, 2022），并取得了显著的性能。这些成果凸显了MoE语言模型的巨大潜力和前景。

尽管MoE架构具有巨大的潜力，现有的MoE架构可能存在知识混杂hybridity和知识冗余redundancy的问题，这限制了专家的专业性，即每个专家获取不重叠且集中的知识（non-overlapping and focused knowledge）。传统的MoE架构通过将Transformer中的前馈网络（FFN）替换为MoE层来实现，每个MoE层由多个专家组成，每个专家的结构与标准FFN相同，每个token会被分配给一个（Fedus et al., 2021）或两个（Lepikhin et al., 2021）专家。该架构存在两个潜在问题：（1）Knowledge Hybridity 知识混杂：现有的MoE实践通常采用有限数量的专家（例如8个或16个），专家数量少，因此分配给某一专家的tokens往往覆盖了多种知识。该专家的参数往往会组合多种不同类型的知识，这些知识难以同时利用。（2）Knowledge Redundancy 知识冗余：分配给不同专家的token可能有一些一样的common知识。多个专家可能在各自的参数中获取相同的知识，从而导致专家参数的冗余。这些问题共同阻碍了现有MoE实践中的专家专长性，防止它们达到MoE模型的理论上限性能。

针对上述问题，我们提出了DeepSeekMoE，一种旨在实现最终专家专长的创新MoE架构。我们的架构包含两个主要策略：（1）Fine-Grained Expert Segmentation：在保持参数数量不变的情况下，我们通过进一步细分FFN的隐藏维度，将专家分割为更细的粒度。相应地，在保持计算成本不变的情况下，我们还激活更多细粒度的专家，从而实现更灵活的激活专家组合。细粒度的专家细分允许将多样化的知识更加精细地分解并精确地学习到不同专家中，使得每个专家保持更高的专长性。此外，激活专家的组合灵活性增加也有助于更准确、针对性的知识获取。（2）Shared Expert Isolation 隔离共享专家：我们将部分专家隔离为共享专家，这些专家始终保持激活，旨在捕捉并整合跨越不同上下文的共同知识。通过将共享知识压缩到这些共享专家中，可以减少其他路由专家之间的冗余。这可以提高参数效率，并确保每个路由专家通过专注于独特的方面保持专业化。这些架构创新为训练一个参数高效的MoE语言模型提供了机会，在该模型中，每个专家都具有高度的专长性。

从2B参数的规模开始，我们验证了DeepSeekMoE架构的优势。我们在12个zero-shot或few-shot基准上进行了评估，涵盖了多种任务。实验证据表明，DeepSeekMoE 2B远超GShard 2B（Lepikhin et al., 2021），甚至与GShard 2.9B相匹配，后者的专家参数和计算量是前者的1.5倍。值得注意的是，我们发现DeepSeekMoE 2B几乎接近其密集模型版本的性能，后者具有相同数量的参数，设定了MoE语言模型的严格上限。为了获得更深入的见解，我们对DeepSeekMoE的专家专长性进行了详细的消融研究和分析。这些研究验证了细粒度专家细分和共享专家隔离的有效性，并提供了实证证据，支持DeepSeekMoE能够实现高度专家专长的观点。

通过利用我们的架构，我们随后将模型参数扩展到16B，并在一个包含2T tokens的大规模语料库上训练了DeepSeekMoE 16B。评估结果表明，在计算量仅为40%的情况下，DeepSeekMoE 16B达到了与DeepSeek 7B（DeepSeek-AI, 2024）相当的性能，后者是一个在同样2T语料库上训练的密集模型。我们还将DeepSeekMoE与开源模型进行了比较，评估结果显示，DeepSeekMoE 16B始终大幅超越了具有类似激活参数数量的模型，并且与LLaMA2 7B（Touvron et al., 2023b）相当，后者的激活参数大约是前者的2.5倍。图1展示了在Open LLM Leaderboard上的评估结果。此外，我们进行监督微调（SFT）以进行对齐，将模型转换为聊天模型。评估结果表明，DeepSeekMoE Chat 16B在聊天场景中的表现与DeepSeek Chat 7B和LLaMA2 SFT 7B相当。受到这些结果的鼓舞，我们进一步进行了初步的工作，将DeepSeekMoE扩展到145B。实验结果仍然一致地验证了它相较于GShard架构的显著优势。此外，使用仅28.5%（甚至可能是18.2%）的计算量，它展现出了与DeepSeek 67B相当的性能。

我们的贡献总结如下：

- **Architectural Innovation**：我们提出了DeepSeekMoE，这是一种创新的MoE架构，旨在实现最终的专家专长性，采用了细粒度专家细分和共享专家隔离两大策略。
  
- **Empirical Validation**：我们进行了广泛的实验，实证验证了DeepSeekMoE架构的有效性。实验结果验证了DeepSeekMoE 2B中专家专长性的高水平，并表明DeepSeekMoE 2B几乎可以接近MoE模型的上限性能。

- **Scalability**：我们将DeepSeekMoE扩展至16B模型，并展示了在计算量仅约40%的情况下，DeepSeekMoE 16B与DeepSeek 7B和LLaMA2 7B达到了可比的性能。我们还进行了初步尝试，将DeepSeekMoE扩展到145B，突显了其在GShard架构上的一致性优势，并展示了与DeepSeek 67B相当的性能。

- **Alignment for MoE**：我们成功地对DeepSeekMoE 16B进行了监督微调，创建了一个对齐的聊天模型，展示了DeepSeekMoE 16B的适应性和多样性。

- **Public Release**：本着开放研究的精神，我们将DeepSeekMoE 16B的模型检查点公开发布。值得注意的是，该模型可以在单个具有40GB内存的GPU上部署，无需量化处理。


## 2. 基础知识：Transformer中的混合专家（MoE）

我们首先介绍在Transformer语言模型中常用的通用MoE架构。标准的Transformer语言模型通过堆叠𝐿层标准的Transformer blocks来构建，其中每个块可以表示如下：

$$
u^l_{1:T} = \text{Self-Att}\left(h^{l-1}_{1:T}\right) + h^{l-1}_{1:T}, \tag{1}
$$

$$
h_{t}^{l} = \text{FFN}\left(u_{t}^{l}\right) + u_{t}^{l}, \tag{2}
$$

其中，𝑇表示序列长度，Self-Att(·)表示自注意力模块，FFN(·)表示前馈网络（Feed-Forward Network，FFN）， $ u^l_{1:T} \in \mathbb{R}^{T \times d}$ 是所有token经过第𝑙层自注意力模块后的隐藏状态， $h_{t}^{l} \in \mathbb{R}^{d}$ 是第𝑡个token经过第𝑙层整个Transformer block后的输出隐藏状态。为简便起见，以上公式中省略了层归一化。

构建MoE语言模型的典型做法通常是在Transformer的指定间隔处将FFN替换为MoE层（Du等，2022；Fedus等，2021；Lepikhin等，2021；Zoph，2022）。MoE层由多个专家组成，每个专家在结构上与标准FFN相同。然后，每个令牌将被分配给一个（Fedus等，2021）或两个（Lepikhin等，2021）专家。如果第𝑙层的FFN被MoE层替换，那么其输出隐藏状态 $h_{t}^{l}$ 的计算可以表示为：

$$
h_t^l = \sum_{i=1}^{N} \left( g_{i,t} \cdot \text{FFN}_i(u_t^l) \right) + u_t^l \tag{3}
$$

$$
g_{i,t} = 
\begin{cases} 
s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N \}, K) \\
0, & \text{otherwise}
\end{cases}  \tag{4}
$$

$$
s_{i,t} = \text{Softmax}_i(u_t^l e_i^l)  \tag{5}
$$

其中，𝑁表示专家的总数，FFN𝑖(·)是第𝑖个专家的FFN， $g_{i,t}$ 表示第𝑖个专家的门控值， $s_{i,t}$ 表示令牌与专家之间的亲和度，Topk(·,𝐾)表示对于第𝑡个令牌和所有𝑁个专家计算出的亲和度中的前𝐾个最高值的集合。$e_i^l$ 是第l层第i个专家的Centroid质心。注意  $g_{i,t}$ 是稀疏的，只有𝐾 out of 𝑁个值是非0的，这保证了计算的高效性。为简便起见，以上公式中省略了层归一化。

<span style="color:red">【TODO】看看具体的工程实现，模型checkout文件中的参数矩阵是dense的，是在训练和推理过程中通过乘0的方式让一部分参数不激活不计算，看看是怎么让具体的乘法计算不发生的</span>


## 3. DeepSeekMoE Architecture

在第2节中概述的通用MoE架构基础上，我们提出了DeepSeekMoE，它专门设计用来发挥专家专长的潜力。如图2所示，我们的架构包含了两个主要策略：细粒度专家细分和共享专家隔离。这两个策略都旨在提高专家专长的水平。

![图1](http://jinluzhang.github.io/assets/posts_img/2025-01-29-DeepSeek/deepseek-MoE-1.png)


### 3.1. Fine-Grained Expert Segmentation

在专家数量有限的情况下，分配给特定专家的tokens更可能涵盖多种类型的知识。因此，指定的专家往往会在其参数中学习到多种截然不同的知识，这些知识很难被同时利用。然而，如果每个token可以被路由到更多的专家，丰富的知识就有可能被分解并分别在不同的专家中学习。在这种情况下，每个专家仍然可以保持较高的专长性，有助于在专家之间分配更集中的知识。

为了实现这一目标，在保持专家参数数量和计算成本一致的情况下，我们对专家进行了更细的划分。更细的专家细分使得激活专家的组合更加灵活和可适应。具体来说，在图(a)所示的典型MoE架构的基础上，我们通过将FFN的中间隐藏维度缩小为原始尺寸的 m 倍，将每个专家FFN分割为 m 个更小的专家。由于每个专家变得更小，相应地，我们还增加了激活专家的数量，增加到原来的 m 倍，以保持相同的计算成本，如图(b)所示。通过细粒度的专家细分，MoE层的输出可以表示为：

$$
h_t^l = \sum_{i=1}^{mN} \left( g_{i,t} \cdot \text{FFN}_i(u_t^l) \right) + u_t^l \tag{6}
$$

$$
g_{i,t} = 
\begin{cases} 
s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq mN \}, mK) \\
0, & \text{otherwise}
\end{cases}  \tag{7}
$$

$$
s_{i,t} = \text{Softmax}_i(u_t^l e_i^l)  \tag{8}
$$

通过细粒度专家细分策略，非零门控的数量也增加到了 $mK$ 。

从组合学的角度来看，细粒度专家细分策略大大增强了激活专家的组合灵活性。举个例子，假设N = 16。典型的top-2路由策略可以产生 $\binom{16}{2} = 120$ 种可能的组合。相比之下，如果每个专家被分割成4个更小的专家，细粒度路由策略可以产生 
$\binom{64}{2} = 4,426,165,368$ 种潜在的组合。组合灵活性的激增增强了实现更准确和针对性知识获取的潜力。

### 3.2. Shared Expert Isolation

在传统的路由策略下，分配给不同专家的tokens可能需要一些共享知识或信息。因此，多个专家可能会在各自的参数中收敛到获取共享知识，从而导致专家参数的冗余。然而，如果存在专门捕捉并整合不同上下文的共享知识的共享专家，那么其他路由专家之间的参数冗余将得到缓解。这种冗余的缓解将有助于构建一个更为参数高效、专家更专注的模型。

为了实现这一目标，除了细粒度专家细分策略外，我们进一步隔离了 $K_s$ 个专家作为共享专家。无论路由模块如何，每个token都会确定性地分配给这些共享专家。为了保持计算成本的恒定，其他路由专家中的激活专家数量将减少 $K_s$ ，如图(c)所示。通过集成共享专家隔离策略，完整的DeepSeekMoE架构中的MoE层可以表示如下：

$$
h_t^l = \sum_{i=1}^{K_s} \text{FFN}_i(u_t^l) + \sum_{i=1}^{mN} \left( g_{i,t} \cdot \text{FFN}_i(u_t^l) \right) + u_t^l \tag{9}
$$

$$
g_{i,t} = 
\begin{cases} 
s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | K_s +1 \leq j \leq mN \}, mK-K_s) \\
0, & \text{otherwise}
\end{cases}  \tag{10}
$$

$$
s_{i,t} = \text{Softmax}_i(u_t^l e_i^l)  \tag{11}
$$

最后，在DeepSeekMoE中，共享专家的数量为 $K_s$ ，总的路由专家数量为 $mN - K_s$ ，而非零门控的数量为 $mK - K_s$。

值得注意的是，共享专家隔离的原型可以归功于Rajbhandari等人（2022）。关键区别在于，他们是从工程角度提出这一策略，而我们则是从算法的角度出发。

### 3.3. Load Balance Consideration

自动学习的路由策略可能会遇到负载不均衡的问题，这表现出两个显著的缺陷。首先，存在路由崩溃的风险（Shazeer等，2017），即模型总是选择少数几个专家，导致其他专家无法得到充分的训练。其次，如果专家分布在多个设备上，负载不均衡可能会加剧计算瓶颈。

**专家级负载均衡损失**  

为了缓解路由崩溃的风险，我们还采用了专家级负载均衡损失。负载均衡损失的计算公式如下：

$$
\mathcal{L}_{ExpBal} = \alpha_1 \sum_{i=1}^{N'} \mathcal{f}_i P_i, \tag{12}
$$

$$
\mathcal{f}_i = \frac{N'}{K'T} \sum_{t=1}^{T} \mathbb{1}(\text{Token} t \text{ selects Expert} i), \tag{13}
$$

$$
P_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t}, \tag{14}
$$

其中， $\alpha_1$ 是一个超参数，称为专家级负载均衡因子； $N'= mN - K_s$ ，$K' = mK - K_s$ 。 $\mathbb{1}(·)$ 表示指示函数。

**Device-Level Balance Loss**  

除了专家级负载均衡损失，我们引入了设备级负载均衡损失。在旨在缓解计算瓶颈时，强制实施严格的专家级负载均衡约束是没有必要的，因为过度约束负载均衡会影响模型性能。相反，我们的主要目标是确保设备之间的计算负载均衡。如果我们将所有路由专家分成 D 组 $\{\mathcal{E}_1, \mathcal{E}_2, ..., \mathcal{E}_D\}$ ，并将每组部署到单个设备上，则设备级负载均衡损失的计算公式如下：

$$
\mathcal{L}_{DevBal} = \alpha_2 \sum_{i=1}^{D} f'_i P'_i, \tag{15}
$$

$$
f'_i = \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i} f_j, \tag{17}
$$

$$
P'_i = \sum_{j \in \mathcal{E}_i} P_j, \tag{16}
$$

其中， $\alpha_2$ 是一个超参数，称为设备级负载均衡因子。在实践中，我们设置一个较小的专家级负载均衡因子，以减轻路由崩溃的风险，同时设置一个较大的设备级负载均衡因子，以促进设备之间的负载均衡计算。


---

# DeepSeek-V2

## Abstract

DeepSeek-V2是混合专家（Mixture-of-Experts, MoE）语言模型，其特点是经济的训练和高效的推理。该模型总共有 2360 亿个参数，其中每个 token 激活 210 亿个参数，并且支持 128K token 的上下文长度。DeepSeek-V2 采用了创新的架构，包括多头潜在注意力（Multi-head Latent Attention, MLA）和 DeepSeekMoE。MLA 通过显著压缩Key-Value缓存为latent向量，保证了高效的推理，而 DeepSeekMoE 则通过稀疏计算sparse computation，使得可以经济高效的训练出strong模型。与 DeepSeek 67B 相比，DeepSeek-V2 在性能上显著提升，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76 倍。我们在一个包含 8.1 万亿 token 的高质量多源语料库上对 DeepSeek-V2 进行了预训练，并进一步进行了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL），充分挖掘其潜力。评估结果显示，即使只有 210 亿激活参数，DeepSeek-V2 及其聊天版本在开源模型中仍然表现出色。

## 1.Introduction

在过去的几年中，大型语言模型（LLMs）（Anthropic, 2023；Google, 2023；OpenAI, 2022, 2023）经历了快速发展，为我们展现了人工通用智能（AGI）的曙光。通常，LLM 的智能随着参数数量的增加而提升，使其能够在各种任务中展示出涌现的能力（Wei et al., 2022）。然而，这种提升伴随着更大计算资源的需求，并可能导致推理吞吐量的下降。这些限制构成了显著的挑战，阻碍了 LLM 的广泛采用和利用。为了应对这一问题，我们提出了 DeepSeek-V2，一款强大的开源混合专家（Mixture-of-Experts, MoE）语言模型，采用创新的 Transformer 架构，使其训练经济、推理高效。该模型总共有 2360 亿参数，其中每个 token 激活 210 亿参数，并支持 128K token 的上下文长度。

我们优化了 Transformer 框架（Vaswani et al., 2017）中的Attention模块和前馈网络Feed-Forward Networks (FFNs)，引入了我们提出的多头潜在注意力（Multi-head Latent Attention, MLA）和 DeepSeekMoE。（1）在注意力机制的背景下，多头注意力（Multi-Head Attention, MHA）（Vaswani et al., 2017）中的键值（Key-Value, KV）缓存对 LLM 的推理效率构成了显著障碍。为了解决这个问题，已经探索了多种方法，包括分组查询注意力（Grouped-Query Attention, GQA）（Ainslie et al., 2023）和多查询注意力（Multi-Query Attention, MQA）（Shazeer, 2019）。然而，这些方法在试图减少 KV 缓存时，往往会牺牲性能。为了实现两者兼得，我们提出了 MLA，这是一种具备低秩键值联合压缩的注意力机制。从实验证据来看，MLA 相比 MHA 能够实现更优的性能，同时在推理过程中显著减少 KV 缓存，从而提升推理效率。（2）对于前馈网络（FFN），我们遵循 DeepSeekMoE 架构（Dai et al., 2024），该架构采用精细的专家分割和共享专家隔离，以提高专家专门化的潜力。与传统的 MoE 架构（如 GShard（Lepikhin et al., 2021））相比，DeepSeekMoE 架构展示了显著的优势，使我们能够以经济的成本训练强大的模型。由于我们在训练过程中采用了专家并行机制，我们还设计了补充机制来控制通信开销并确保负载均衡。通过结合这两种技术，DeepSeek-V2 同时具备了强大的性能、经济的训练成本和高效的推理吞吐量。

我们构建了一个高质量、多源的预训练语料库，包含 8.1 万亿 tokens。与 DeepSeek 67B（我们之前发布的版本）（DeepSeek-AI, 2024）所使用的语料库相比，这个语料库的数据量有所增加，尤其是中文数据，并且数据质量更高。我们首先在完整的预训练语料库上对 DeepSeek-V2 进行预训练。然后，我们收集了 150 万对话会话，涵盖了数学、代码、写作、推理、安全等多个领域，用于对 DeepSeek-V2 Chat 进行监督微调（Supervised Fine-Tuning, SFT）。最后，我们参照 DeepSeekMath（Shao et al., 2024）采用群体相对策略优化（Group Relative Policy Optimization, GRPO）进一步调整模型，以使其更符合人类偏好，并产生 DeepSeek-V2 Chat（RL）。

我们在英语和中文的一系列基准测试中评估了 DeepSeek-V2，并与代表性的开源模型进行了比较。评估结果显示，即使只有 210 亿个激活参数，DeepSeek-V2 仍然在开源模型中实现了顶级表现，成为最强的开源 MoE 语言模型。图 1(a) 显示，在 MMLU 测试中，DeepSeek-V2 在仅激活少量参数的情况下仍取得了领先的表现。此外，如图 1(b) 所示，与 DeepSeek 67B 相比，DeepSeek-V2 节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提升了 5.76 倍。我们还在开放式基准测试中评估了 DeepSeek-V2 Chat（SFT）和 DeepSeek-V2 Chat（RL）。值得注意的是，DeepSeek-V2 Chat（RL）在 AlpacaEval 2.0（Dubois et al., 2024）上的长度控制胜率为 38.9%，在 MT-Bench（Zheng et al., 2023）上的整体得分为 8.97，在 AlignBench（Liu et al., 2023）上的整体得分为 7.91。英语开放式对话评估表明，DeepSeek-V2 Chat（RL）在开源聊天模型中具有顶级表现。此外，在 AlignBench 上的评估结果表明，在中文环境下，DeepSeek-V2 Chat（RL）超过了所有开源模型，甚至超过了大多数闭源模型。

为了促进 MLA 和 DeepSeekMoE 的进一步研究与开发，我们还发布了 DeepSeek-V2-Lite，这是一个较小的模型，配备了 MLA 和 DeepSeekMoE，供开源社区使用。该模型总共有 157 亿个参数，每个 token 激活 24 亿个参数。关于 DeepSeek-V2-Lite 的详细描述可以在附录 B 中找到。

本文的其余部分将首先详细描述 DeepSeek-V2 的模型架构（第 2 节）。随后，我们将介绍我们的预训练工作，包括训练数据的构建、超参数设置、基础设施、长上下文扩展以及模型性能和效率的评估（第 3 节）。接下来，我们将展示我们在对齐方面的努力，包括监督微调（SFT）、强化学习（RL）、评估结果和其他讨论（第 4 节）。最后，我们总结结论，讨论 DeepSeek-V2 的当前局限性，并概述我们的未来工作（第 5 节）。



## 2.Architecture

总体来说，DeepSeek-V2 仍然采用了 Transformer 架构（Vaswani 等人，2017），其中每个 Transformer 模块由一个注意力模块和一个前馈网络（FFN）组成。然而，针对注意力模块和 FFN，我们设计并采用了创新的架构。对于注意力，我们设计了 MLA，它利用低秩的键值联合压缩来消除推理时键值缓存的瓶颈，从而支持高效的推理。对于 FFN，我们采用了 DeepSeekMoE 架构（Dai 等人，2024），这是一种高性能的 MoE 架构，能够以经济的成本训练出强大的模型。DeepSeek-V2 的架构示意图如图所示，我们将在本节中介绍 MLA 和 DeepSeekMoE 的详细信息。至于其他一些细节（例如层归一化和 FFN 中的激活函数），除非特别说明，DeepSeek-V2 遵循 DeepSeek 67B（DeepSeek-AI，2024）的设置。

![图1](http://jinluzhang.github.io/assets/posts_img/2025-01-29-DeepSeek/deepseek-v2-1.png)

### 2.1.Multi-Head Latent Attention

传统的 Transformer 模型通常采用多头注意力（MHA）（Vaswani 等人，2017），但在生成过程中，其庞大的键值缓存会成为限制推理效率的瓶颈。为了减少 KV 缓存，提出了多查询注意力（MQA）（Shazeer，2019）和分组查询注意力（GQA）（Ainslie 等人，2023）。这些方法需要更小的 KV 缓存，但它们的性能无法与 MHA 匹敌（我们在附录 D.1 中提供了 MHA、GQA 和 MQA 的消融实验）。

对于 DeepSeek-V2，我们设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。MLA 通过低秩键值联合压缩，实现了比 MHA 更好的性能，同时需要显著更少的 KV 缓存。我们将在以下部分介绍其架构，并在附录 D.2 中提供 MLA 与 MHA 的对比。

![图1](http://jinluzhang.github.io/assets/posts_img/2025-01-29-DeepSeek/deepseek-v2-2.png)


#### 2.1.1 标准的多头注意力

我们首先介绍标准的 MHA 机制作为背景。记 $d$ 表示嵌入维度（embedding dimension），$n_h$ 表示注意力头的数量，$d_h$ 表示每个注意力头的维度，$\mathbf{h}_t \in \mathbb{R}^d$ 表示给定的注意力层中第t个token对应的attention输入。标准 MHA 首先通过三个矩阵 $W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}$ 分别生成 $\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t \in \mathbb{R}^{d_h n_h}$ :

$$
\mathbf{q}_t = W^Q \mathbf{h}_t, \tag{1}
$$

$$
\mathbf{k}_t = W^K \mathbf{h}_t, \tag{2}
$$

$$
\mathbf{v}_t = W^V \mathbf{h}_t, \tag{3}
$$

然后，$\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t$ 会切成 $n_h$ 块，分别送入 $n_h$ 个多个头做计算。

$$
[\mathbf{q}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{q}_{t,n_h}] = \mathbf{q}_t,
$$

$$
[\mathbf{k}_{t,1}; \mathbf{k}_{t,2}; \dots; \mathbf{k}_{t,n_h}] = \mathbf{k}_t,
$$

$$
[\mathbf{v}_{t,1}; \mathbf{v}_{t,2}; \dots; \mathbf{v}_{t,n_h}] = \mathbf{v}_t, \tag{4}
$$

以第i个Attention头为例，记它第t个token对应的的k向量 $\mathbf{k}^i$ 如下，v类似。

$$
\mathbf{k}_{*,i} = [\mathbf{k}_{1,i}; \mathbf{k}_{2,i}; \dots; \mathbf{k}_{t,i}],
$$

$$
\mathbf{v}_{*,i} = [\mathbf{v}_{1,i}; \mathbf{v}_{2,i}; \dots; \mathbf{v}_{t,i}], \tag{5}
$$

然后， $\mathbf{q}_{t,i}$ 要和所有keys计算MatMul，除以 $\sqrt{d_h}$ 后计算softmax，代表对所有values的权重。

记 $\mathbf{q}_{t,i}$ 对应的输出是 $\mathbf{o}_{t,i}$ ：

$$
 \mathbf{o}_{t,i} = \text{Softmax}\left(\frac{\mathbf{q}_{t,i} \mathbf{k}_{*,i}}{\sqrt{d_h}}\right) \mathbf{v}_{*,i}, \tag{6}
$$

然后，所有 $\mathbf{o}_{t,i}$ concat在一起后乘以$W^O \in \mathbb{R}^{d \times d_h n_h}$ ，就是第t个token对应的MHA的整体输出：

$$
\mathbf{u}_t = W^O [\mathbf{o}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{o}_{t,n_h}], \tag{7}
$$

在推理过程中，所有的 $\mathbf{k_t}, \mathbf{v_t}$ 需要缓存以加速推理，因此在推理时，每层需要缓存 2×𝑑×sequence_length 个元素。在模型部署时，这种庞大的 KV 缓存成为了限制maximum batch和sequence length的瓶颈。

<span style="color:red">参考llama中的Attention模块详细拆解图。 https://jinluzhang.github.io/%E6%8A%80%E6%9C%AF/2023/08/02/LLAMA2-code.html 。比如我们在第T步的输入是500个token，输出了第501个token，那么继续自回归inference，在第T+1步要输入501个token，此时应该要计算这个501个token的WQ、WK、WV矩阵乘法、501个key和501个value之间的score，但前面的500个token的已经算过了，为了减少计算量，会把他们缓存下来。但这个缓存比较大，是dim X token_length大小的，本文的MLA可以减少这部分的缓存空间。注意在第T+1步中，我们的输出是第502个token，query是已知的501个token，但在这一步，只有第501个token的score是没计算过的，所以在这一步中的query一般就只记做第501个token它自己。以及，按逻辑每一层的output也要缓存的。但因为kv缓存在，下一层真正用到的未知的就只有第501个token对应的那个输出，所以output就不需要缓存了</span>

#### 2.1.2 Low-Rank Key-Value Joint Compression

MLA 的核心是对键和值进行低秩联合压缩low-rank joint compression以减少 KV 缓存：

$$
\mathbf{c}_t^{KV} = W^{DKV} \mathbf{h}_t, \tag{8}
$$

$$
\mathbf{k}_t^C = W^{UK} \mathbf{c}_t^{KV}, \tag{9}
$$

$$
\mathbf{v}_t^C = W^{UV} \mathbf{c}_t^{KV}, \tag{10}
$$
 
其中， $\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}$ 是压缩后的latent向量， $W^{DKV} \in \mathbb{R}^{d_c \times d}$ 为down-projection下投影矩阵； $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别为key和value的上投影矩阵。

在推理过程中，MLA 仅需要缓存 $\mathbf{c}_t^{KV}$ ，因此其 KV 缓存只有 $d_c$ ×sequence_length个元素。
此外，在推理时，由于 $W^{UK}$ 可以合并进 $W^Q$ 中，$W^{UV}$ 可以合并进 $W^O$ 中，因此我们甚至无需付出额外更多的计算。

<span style="color:red">原本第一反应这是时间换空间，在MHA里，$W^K \mathbf{h}_t$ 和 $W^V \mathbf{h}_t$ 是不用计算的，结果已经被缓存了。但是MLA需要多算两次up-projection： $W^{UK} \mathbf{c}_t^{KV}$ 、 $W^{UV} \mathbf{c}_t^{KV}$  。 但巧妙在参数矩阵可以合并，并且合并后，对应的 $W^Q$ 和 $W^O$ size也降低了，和MHA相比inference时候的计算量也降低了。（训练的时候没有做这个合并，所以逻辑上参数空间没有被压缩，模型的表达能力没有被压缩）。</span>

为了减少训练过程中的激活内存，我们还对query进行了低秩压缩（就是对 $W^Q$ 做了低秩压缩分解，实际上参数量变少了，模型表达能力按道理也降低了），尽管这不能减少 KV 缓存：

$$
\mathbf{c}_t^Q = W^{DQ} \mathbf{h}_t, \tag{11}
$$

$$
\mathbf{q}_t^C = W^{UQ} \mathbf{c}_t^Q, \tag{12}
$$


#### 2.1.3. Decoupled Rotary Position Embedding

延续 DeepSeek 67B（DeepSeek-AI，2024），我们想在 DeepSeek-V2 中也使用旋转位置嵌入（Rotary Position Embedding，RoPE）（Su 等人，2024）。然而，RoPE 与低秩键值压缩不兼容。具体而言，RoPE 对于keys和queries都是位置敏感的。如果我们将 RoPE 应用于 $\mathbf{k}_t^C$ ，那么公式（9）中的 $W^{UK}$ 将与一个位置敏感的 RoPE 矩阵耦合。这样，在推理过程中，它就无法再被合并到 $W^Q$ 中，因为与当前正在生成的token相关的 RoPE 矩阵位于 $W^Q$ 和 $W^{UK}$ 之间，而矩阵乘法并不满足交换律。因此，我们必须在推理过程中重新计算所有前序token的keys，这将显著影响推理效率。

为了解决这个问题，我们提出了一种解耦 RoPE 策略，该策略使用额外的多头queries  $\mathbf{q}_{t,i}^R \in \mathbb{R}^{d_h^R}$ 和一个共享key  $\mathbf{k}_t^R \in \mathbb{R}^{d_h^R}$ 来携带RoPE信息，其中 $d_h^R$ 代表每个头中，解耦了的queries和key的维度大小。通过 RoPE 解耦策略，MLA 执行以下计算：

$$
 [ \mathbf{q}_{t,1}^R; \mathbf{q}_{t,2}^R; \dots; \mathbf{q}_{t,n_h}^R] = \mathbf{q}_t^R = \text{RoPE}(W^{QR} \mathbf{c}_t^Q) = \text{RoPE}(W^{QR} W^{DQ} \mathbf{h}_t), \tag{13}
$$

$$
\mathbf{k}_t^R = \text{RoPE}(W^{KR} \mathbf{h}_t), \tag{14}
$$

$$
\mathbf{q}_{t,i} = [\mathbf{q}_{t,i}^C; \mathbf{q}_{t,i}^R],
$$

$$
\mathbf{k}_{t,i} = [\mathbf{k}_{t,i}^C; \mathbf{k}_{t}^R], \tag{15}
$$

$$
 \mathbf{o}_{t,i} = \text{Softmax}\left(\frac{\mathbf{q}_{t,i} \mathbf{k}_{*,i}}{\sqrt{d_h + d_h^R}}\right) \mathbf{v}_{*,i}^C, \tag{16}
$$

$$
\mathbf{u}_t = W^O [\mathbf{o}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{o}_{t,n_h}], \tag{17}
$$

在推理过程中，解耦的key也需要缓存，即公式14对应的 $\mathbf{k}_t^R$ 。


### 2.2. DeepSeekMoE: Training Strong Models at Economical Costs

#### 2.2.1. Basic Architecture

见本文单独的DeepSeekMoE论文部分，内容更详细些。

#### 2.2.2. Device-Limited Routing

我们设计了一种设备限制路由机制，以限制MoE相关的通信成本。当采用专家并行时，路由专家将分布在多个设备上。对于每个token，其MoE相关的通信频率与其目标专家所覆盖的设备数量成正比。由于DeepSeekMoE中的细粒度专家细分，激活的专家数量可能很大，因此如果应用专家并行，MoE相关的通信成本将增加。

对于DeepSeek-V2，除了简单的top-K路由专家选择外，我们还确保每个token的目标专家最多分布在 M 个设备上。具体来说，对于每个token，我们首先选择 M 个设备，这些设备中包含亲和度分数最高的专家。然后，我们在这些 M 个设备上执行top-K选择。实际上，我们发现当 $M \geq 3$ 时，设备限制路由可以实现与无限制top-K路由大致一致的良好性能。

#### 2.2.3. Auxiliary Loss for Load Balance

我们考虑了自动学习的路由策略中的负载均衡问题。首先，负载不均衡将增加路由崩溃的风险（Shazeer等人，2017），阻止一些专家得到充分训练和利用。其次，当采用专家并行时，负载不均衡将降低计算效率。在DeepSeek-V2的训练过程中，我们设计了三种辅助损失，分别用于控制专家级负载均衡（LExpBal）、设备级负载均衡（LDevBal）和通信均衡（LCommBal）。

前两种损失见前文DeepSeekMoE论文部分。

**Communication Balance Loss**  
最后，我们引入了通信均衡损失，以确保每个设备的通信负载均衡。尽管设备限制路由机制保证了每个设备的发送通信是有上限的，但如果某个设备接收到的token数量超过其他设备，实际的通信效率也会受到影响。为了减轻这个问题，我们设计了一个通信均衡损失，公式如下：

$$
\mathcal{L}_{CommBal} = \alpha_3 \sum_{i=1}^{D} f''_i P''_i, \tag{29}
$$

$$
\mathcal{f}''_i = \frac{D}{MT} \sum_{t=1}^{T} \mathbb{1}(\text{Token} t \text{is sent to Device} i), \tag{30}
$$

$$
P''_i = \sum_{j \in \mathcal{E}_i} P_j, \tag{31}
$$

其中， $\alpha_3$ 是一个超参数，称为通信均衡因子。设备限制路由机制的操作原则是确保每个设备最多向其他设备传输𝑀𝑇个隐藏状态。与此同时，通信均衡损失也鼓励每个设备从其他设备接收大约𝑀𝑇个隐藏状态。通信均衡损失确保了设备之间信息交换的均衡，促进了高效的通信。

#### 2.2.4. Token-Dropping Strategy

虽然负载均衡损失旨在鼓励负载均衡，但需要注意的是，它们无法保证严格的负载均衡。为了进一步减轻由于负载不均衡导致的计算浪费，我们在训练过程中引入了设备级的token丢弃策略。该方法首先计算每个设备的平均计算预算，这意味着每个设备的容量因子等于1.0。然后，受到Riquelme等人（2021年）的启发，我们丢弃每个设备上亲和度最低的tokens，直到达到计算预算。此外，我们确保大约10%的训练序列中的tokens永远不会被丢弃。通过这种方式，我们可以根据推理过程中的效率要求灵活地决定是否丢弃tokens，并始终确保训练与推理之间的一致性。

## 3. Pre-Training

### 3.1. Experimental Setups

#### 3.1.1. Data Construction

在保持与DeepSeek 67B（DeepSeek-AI，2024）相同的数据处理阶段的基础上，我们扩展了数据量并提升了数据质量。为了扩大我们的预训练语料库，我们探索了互联网数据的潜力，并优化了数据清洗过程，从而恢复了大量误删的数据。此外，我们还加入了更多中文数据，旨在更好地利用中文互联网可用的语料库。除了数据量，我们还关注数据质量。我们通过多种来源的高质量数据来丰富我们的预训练语料库，并同时改进了基于质量的过滤算法。改进后的算法确保会移除大量无益数据，同时大部分有价值的数据将被保留。此外，我们从预训练语料库中剔除了有争议的内容，以减少特定区域文化所引入的数据偏差。 

我们采用与DeepSeek 67B相同的分词器，该分词器基于字节级字节对编码（BBPE）算法构建，词表大小为100K。我们的分词预训练语料库包含8.1万亿个token，其中中文token比英文token多约12%。

#### 3.1.2. Hyper-Parameters

**Model Hyper-Parameters**

我们将Transformer层数设置为60，隐藏维度设置为5120。所有可学习的参数使用标准差为0.006的方式进行随机初始化。在MLA中，我们将注意力头数 $n_h$ 设置为128，每个头的维度 $d_h$ 设置为128。KV压缩维度 $d_c$ 设置为512，查询压缩维度 $d_c'$ 设置为1536。对于解耦查询和键，我们将每个头的维度 $d_R$ 设置为64。

根据Dai等（2024）的研究，我们将除了第一层之外的所有FFN替换为MoE层。每个MoE层由2个共享专家和160个路由专家组成，每个专家的中间隐藏维度为1536。在路由专家中，每个token将激活6个专家。此外，低秩压缩和细粒度专家细分将影响层的输出规模。因此，实际上，我们在压缩的潜在向量后面使用了额外的RMS Norm层，并在宽度瓶颈处（即压缩的潜在向量和路由专家的中间隐藏状态）乘上额外的缩放因子，以确保训练的稳定性。在此配置下，DeepSeek-V2包含236B总参数，其中每个token激活21B参数。

**Training Hyper-Parameters**

我们使用AdamW优化器（Loshchilov和Hutter，2017），其超参数设置为 $\beta_1 = 0.9$ ， $\beta_2 = 0.95$ ，权重衰减为0.1。学习率使用热身和逐步衰减策略进行调度（DeepSeek-AI，2024）。最初，学习率在前2K步内线性从0增加到最大值。随后，学习率在训练约60%的tokens后乘以0.316，在训练约90%的tokens后再乘以0.316。最大学习率设置为2.4 × 10⁻⁴，梯度裁剪范数设置为1.0。我们还使用了批量大小调度策略，其中在前225B tokens的训练过程中，批量大小从2304逐渐增加到9216，然后在剩余的训练中保持9216。我们将最大序列长度设置为4K，并在8.1T的tokens上训练DeepSeek-V2。我们利用管道并行性将模型的不同层部署到不同的设备上，对于每一层，路由专家将均匀部署到8个设备上（D = 8）。对于设备限制路由，每个token将被发送到最多3个设备上（ M = 3 ）。关于负载均衡损失，我们将 $\alpha_1$ 设置为0.003， $\alpha_2$ 设置为0.05， $\alpha_3$ 设置为0.02。在训练过程中我们使用token丢弃策略来加速，但在评估时不丢弃任何token。

#### 3.1.3. Infrastructures

DeepSeek-V2是在HAI-LLM框架（High-flyer，2023）上训练的，这是由我们工程师内部开发的一个高效且轻量的训练框架。它采用16路zero-bubble pipeline parallelism（Qi等，2023）、8路expert parallelism（Lepikhin等，2021）和ZeRO-1数据并行（Rajbhandari等，2020）。由于DeepSeek-V2激活的参数相对较少，并且部分操作会重新计算以节省激活内存，因此可以在不需要张量并行的情况下进行训练，从而减少通信开销。此外，为了进一步提高训练效率，我们将共享专家的计算与专家并行的all-to-all通信进行重叠。我们还定制了更快的CUDA内核，用于不同专家之间的通信、路由算法和融合线性计算。此外，MLA还基于改进版的FlashAttention-2（Dao，2023）进行了优化。

我们在配备NVIDIA H800 GPU的集群上进行所有实验。H800集群中的每个节点包含8个GPU，这些GPU通过节点内部的NVLink和NVSwitch连接。节点间使用InfiniBand互连进行通信。

#### 3.1.4. Long Context Extension

在DeepSeek-V2的初步预训练之后，我们采用YaRN（Peng等，2023）将默认的上下文窗口长度从4K扩展到128K。YaRN特别应用于解耦共享键 $k_t^R$ ，因为它负责携带RoPE（Su等，2024）。对于YaRN，我们设置了比例 s 为40， $\alpha$ 为1， $\beta$ 为32，目标最大上下文长度为160K。在这些设置下，我们可以预期模型能够很好地响应128K的上下文长度。略微偏离原始YaRN，由于我们的独特注意力机制，我们调整了长度缩放因子 $\sqrt{t}$ 来调节注意力熵。因子 t 计算为 $\sqrt{t} = 0.0707 \ln s + 1$ ，旨在最小化困惑度perplexity。

我们还额外训练了1000步，序列长度为32K，批量大小为576个序列。尽管训练仅在32K的序列长度下进行，但当在128K的上下文长度下进行评估时，模型仍然表现出强大的性能。



## 例子-DeepSeek-V2-Lite

以DeepSeek-V2-Lite模型为例，模型定义：https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py

关键参数如下。

模型Total Params=16B，Activated Params=2.4B，Context Length=32k

```python

        vocab_size=102400,
        hidden_size=4096,
        intermediate_size=11008,
        moe_intermediate_size = 1407,
        num_hidden_layers=30,
        num_attention_heads=32,
        num_key_value_heads=32,
        n_shared_experts = None,
        n_routed_experts = None,
        ep_size = 1,
        routed_scaling_factor = 1.0,
        kv_lora_rank = 512,
        q_lora_rank = 1536,
        qk_rope_head_dim = 64,
        v_head_dim = 128,
        qk_nope_head_dim = 128,
        topk_method = 'gready',
        n_group = None,
        topk_group = None,
        num_experts_per_tok = None,
        moe_layer_freq = 1,
        first_k_dense_replace = 0,
        norm_topk_prob = False,
        scoring_func = 'softmax',
        aux_loss_alpha = 0.001,
        seq_aux = True,
        hidden_act="silu",
        max_position_embeddings=2048,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=None,
        bos_token_id=100000,
        eos_token_id=100001,
        pretraining_tp=1,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        rope_scaling=None,
        attention_bias=False,
        attention_dropout=0.0

{
  "architectures": [
    "DeepseekV2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_deepseek.DeepseekV2Config",
    "AutoModel": "modeling_deepseek.DeepseekV2Model",
    "AutoModelForCausalLM": "modeling_deepseek.DeepseekV2ForCausalLM"
  },
  "aux_loss_alpha": 0.001,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "first_k_dense_replace": 1,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 10944,
  "kv_lora_rank": 512,
  "max_position_embeddings": 163840,
  "model_type": "deepseek_v2",
  "moe_intermediate_size": 1408,
  "moe_layer_freq": 1,
  "n_group": 1,
  "n_routed_experts": 64,
  "n_shared_experts": 2,
  "norm_topk_prob": false,
  "num_attention_heads": 16,
  "num_experts_per_tok": 6,
  "num_hidden_layers": 27,
  "num_key_value_heads": 16,
  "pretraining_tp": 1,
  "q_lora_rank": null,
  "qk_nope_head_dim": 128,
  "qk_rope_head_dim": 64,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 40,
    "mscale": 0.707,
    "mscale_all_dim": 0.707,
    "original_max_position_embeddings": 4096,
    "type": "yarn"
  },
  "rope_theta": 10000,
  "routed_scaling_factor": 1.0,
  "scoring_func": "softmax",
  "seq_aux": true,
  "tie_word_embeddings": false,
  "topk_group": 1,
  "topk_method": "greedy",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.33.1",
  "use_cache": true,
  "v_head_dim": 128,
  "vocab_size": 102400
}
```

模型结构及每一个参数矩阵的具体size如下。layer.1到layer.26结构一样；每个layer里，experts.0到experts.63结构一样。

```python
{
  "metadata": {
    "total_size": 31412968448
  },
  "weight_map": {
    "model.embed_tokens.weight": {"dtype":"BF16","shape":[102400,2048]},
    "model.norm.weight": {"dtype":"BF16","shape":[2048]},
    "lm_head.weight": {"dtype":"BF16","shape":[102400,2048]},

    "model.layers.0.self_attn.q_proj.weight": {"dtype":"BF16","shape":[3072,2048]},
    "model.layers.0.self_attn.kv_a_proj_with_mqa.weight": {"dtype":"BF16","shape":[576,2048]},
    "model.layers.0.self_attn.kv_a_layernorm.weight": {"dtype":"BF16","shape":[512]},
    "model.layers.0.self_attn.kv_b_proj.weight": {"dtype":"BF16","shape":[4096,512]},
    "model.layers.0.self_attn.o_proj.weight": {"dtype":"BF16","shape":[2048,2048]},
    "model.layers.0.mlp.gate_proj.weight": {"dtype":"BF16","shape":[10944,2048]},
    "model.layers.0.mlp.up_proj.weight": {"dtype":"BF16","shape":[10944,2048]},
    "model.layers.0.mlp.down_proj.weight": {"dtype":"BF16","shape":[2048,10944]},
    "model.layers.0.input_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    "model.layers.0.post_attention_layernorm.weight": {"dtype":"BF16","shape":[2048],
    "model.layers.1.self_attn.q_proj.weight": {"dtype":"BF16","shape":[3072,2048]},
    "model.layers.1.self_attn.kv_a_proj_with_mqa.weight": {"dtype":"BF16","shape":[576,2048]},
    "model.layers.1.self_attn.kv_a_layernorm.weight": {"dtype":"BF16","shape":[512]},
    "model.layers.1.self_attn.kv_b_proj.weight": {"dtype":"BF16","shape":[4096,512]},
    "model.layers.1.self_attn.o_proj.weight": {"dtype":"BF16","shape":[2048,2048]},
    "model.layers.1.mlp.gate.weight": {"dtype":"BF16","shape":[64,2048]},
    "model.layers.1.mlp.shared_experts.gate_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.1.mlp.shared_experts.up_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.1.mlp.shared_experts.down_proj.weight": {"dtype":"BF16","shape":[2048,2816]},
    "model.layers.1.mlp.experts.0.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.0.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.0.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
    "model.layers.1.mlp.experts.1.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.1.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.1.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
     ...
    "model.layers.1.mlp.experts.63.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.63.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.1.mlp.experts.63.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},
    "model.layers.1.input_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    "model.layers.1.post_attention_layernorm.weight": {"dtype":"BF16","shape":[2048]},
    ...
    "model.layers.26.mlp.shared_experts.gate_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.26.mlp.shared_experts.up_proj.weight": {"dtype":"BF16","shape":[2816,2048]},
    "model.layers.26.mlp.shared_experts.down_proj.weight": {"dtype":"BF16","shape":[2048,2816]},
    ...
    "model.layers.26.mlp.experts.63.gate_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.26.mlp.experts.63.up_proj.weight": {"dtype":"BF16","shape":[1408,2048]},
    "model.layers.26.mlp.experts.63.down_proj.weight": {"dtype":"BF16","shape":[2048,1408]},

```

---


# DeepSeek-V3

## Abstract

DeepSeek-V3是混合专家（MoE）语言模型，总参数量6710亿，其中每个token激活参数量为370亿。为实现高效推理与经济训练，DeepSeek-V3沿用了在DeepSeek-V2中经过充分验证的多头潜在注意力（MLA Multi-head Latent Attention）架构与DeepSeekMoE架构，并开创性地引入无辅助损失的负载均衡策略（an auxiliary-loss-free strategy for load balancing），同时采用multi-token prediction训练目标以提升模型性能。我们基于14.8万亿高质量多源语料对DeepSeek-V3进行预训练，并通过监督微调与强化学习阶段充分释放其能力。综合评估表明，DeepSeek-V3不仅显著超越其他开源模型，性能更可媲美顶尖闭源模型。该模型的完整训练仅需278.8万H800 GPU小时，且训练过程异常稳定，全程未发生任何不可恢复的损失尖峰，也未进行任何回滚操作。
模型检查点已发布于：https://github.com/deepseek-ai/DeepSeek-V3

## 1.Introduction

近年来，大型语言模型（LLM）经历了快速迭代与进化（Anthropic, 2024；Google, 2024；OpenAI, 2024a），不断逼近通用人工智能（AGI）的边界。除闭源模型外，以DeepSeek系列（DeepSeek-AI, 2024a,b,c；Guo等，2024）、LLaMA系列（AI@Meta, 2024a,b；Touvron等，2023a,b）、Qwen系列（Qwen, 2023, 2024a,b）及Mistral系列（Jiang等，2023；Mistral, 2024）为代表的开源模型也取得显著进展，持续缩小与闭源模型的差距。为进一步突破开源模型能力边界，我们通过scale up 推出DeepSeek-V3——总参数量达6710亿的混合专家（MoE）模型，其中每个token激活参数量为370亿。

基于前瞻性视角，我们始终追求卓越性能与经济成本的平衡。因此，在架构层面，为了实现高效推理，沿用多头潜在注意力（MLA）（DeepSeek-AI, 2024c），为了高效训练，仍沿用DeepSeekMoE架构（Dai等，2024）。这两项技术经过了DeepSeek-V2验证。在此基础上，我们引入两项创新策略：首先，DeepSeek-V3 引入了一种不依赖辅助损失（auxiliary-loss-free）的负载均衡策略（Wang 等, 2024a），旨在最大程度地降低为实现负载均衡而对模型性能产生的负面影响。其次，DeepSeek-V3 采用了多 token 预测的训练目标，我们观察到这一策略有助于提升在各项评测基准上的整体表现。

通过FP8混合精度训练框架实现训练加速与显存优化。低精度训练作为高效训练的有效方案（Dettmers等，2022；Kalamkar等，2019；Narang等，2017；Peng等，2023b），其发展与硬件能力演进密切关联（Luo等，2024；Micikevicius等，2022；Rouhani等，2023a）。本研究首次在超大规模模型上验证FP8训练的有效性，通过FP8计算与存储支持实现训练速度提升与显存占用降低。

为了实现高效训练，我们支持 FP8 混合精度训练，并对训练框架进行全面优化。低精度训练已经成为提升训练效率的有力解决方案（Dettmers 等, 2022；Kalamkar 等, 2019；Narang 等, 2017；Peng 等, 2023b），其发展与硬件能力的进步密切相关（Luo 等, 2024；Micikevicius 等, 2022；Rouhani 等, 2023a）。在本研究中，我们首次在超大规模模型上引入并验证了 FP8 混合精度训练框架的有效性。通过支持 FP8 的计算与存储，我们不仅加速了训练速度，还减少了 GPU 内存的占用。在训练框架方面，我们设计了 DualPipe 算法以实现高效的流水线并行。该算法减少了pipeline bubbles，并通过计算与通信的重叠（computation-communication overlap）在训练过程中隐藏了大部分通信开销。这种方式可以保证在模型规模进一步扩张时，只要维持恒定的计算与通信比率，仍能在跨节点间使用细粒度专家fine-grained experts，并将 all-to-all 通信的开销降至近乎为零。此外，我们还开发了高效的跨节点 all-to-all 通信内核，以充分利用 InfiniBand (IB) 与 NVLink 的带宽。我们也对内存占用进行了精细化优化，使得在不依赖高成本张量并行的情况下，依然可以训练 DeepSeek-V3。通过这些努力，我们实现了高效的训练过程。

在预训练阶段，我们使用 14.8T 高质量且多样化的 token 对 DeepSeek-V3 进行训练，整个预训练过程十分稳定。期间没有出现无法恢复的损失爆增，也无需回退。接下来，我们对 DeepSeek-V3 进行了两阶段的上下文长度扩展：第一阶段将最大上下文长度扩展至 32K，第二阶段进一步扩展至 128K。随后，我们对 DeepSeek-V3 的基础模型进行后训练（post-training），包括有监督微调（SFT）和强化学习（RL），以使其更贴合人类偏好并进一步释放模型潜力。在后训练阶段，我们从 DeepSeek-R1 系列模型中蒸馏其推理能力，并在此过程中谨慎地平衡模型的准确性与生成长度。

我们在大量基准上对 DeepSeek-V3 进行了全面评估。虽然训练成本较为经济，但综合评测结果显示，DeepSeek-V3-Base 已成为目前最强的开源基础模型，尤其在代码与数学方面表现突出。其聊天版本在多项标准和开放式基准上也优于其他开源模型，并在性能上可比肩主流闭源模型（包括 GPT-4o 和 Claude-3.5-Sonnet）。

最后，我们再次强调 DeepSeek-V3 所具有的经济训练成本（见表 1），这是通过算法、框架与硬件的协同优化共同实现的。在预训练阶段，DeepSeek-V3 每训练 1 万亿个 token 仅需消耗 18 万个 H800 GPU 小时；换言之，在拥有 2048 张 H800 GPU 的集群上只需运行 3.7 天即可完成。这使得整个预训练阶段在不到两个月的时间内完成，耗费了共 266.4 万 GPU 小时。再加上上下文长度扩展所用的 11.9 万 GPU 小时以及后训练所需的 0.5 万 GPU 小时，DeepSeek-V3 的完整训练总耗时仅为约 278.8 万 GPU 小时。若按每 GPU 小时 2 美元的租用价格计算，总训练成本仅为 557.6 万美元。需要注意的是，上述成本仅包括 DeepSeek-V3 的正式训练，不包含在模型架构、算法或数据等方面进行前期研究和消融实验的费用。

核心创新贡献如下：

**Architecture: Innovative Load Balancing Strategy and Training Objective**

 - 在 DeepSeek-V2 高效架构的基础上，我们率先提出了一种不依赖辅助损失的负载均衡策略，最大限度地减少了为实现负载均衡而导致的模型性能损失。

 - 我们探索了Multi-Token Prediction（MTP）目标，并证明它可以提高模型性能。同时，该目标也可用于推理加速中的推测解码（speculative decoding）。

**Pre-Training: Towards Ultimate Training Efficiency**

 - 我们设计了一套 FP8 混合精度训练框架，并首次在超大规模模型上验证了 FP8 训练的可行性与有效性。

 - 通过算法、框架与硬件的协同设计，我们突破了跨节点 MoE 训练中的通信瓶颈，实现了几乎完全的计算与通信重叠。此举不仅大幅提升了训练效率并降低了训练成本，也使得我们可以在不增加额外开销的情况下进一步扩大模型规模。

 - 仅用 266.4 万个 H800 GPU 小时的经济成本，我们完成了 DeepSeek-V3 在 14.8 万亿条文本上的预训练，打造了目前最强的开源基础模型。随后的后续训练阶段仅需 10 万个 GPU 小时。

**Post-Training: Knowledge Distillation from DeepSeek-R1**

 - 我们提出了一种创新方法，将CoT模型（Chain-of-Thought,DeepSeek-R1 系列模型之一）中的推理能力蒸馏到标准大语言模型中，尤其是 DeepSeek-V3。该流程将 R1 模型的验证与反思模式优雅地融合至 DeepSeek-V3，显著提升了其推理性能。同时，我们也对 DeepSeek-V3 的输出风格与长度加以控制。

**核心评测结果概览**

 - 知识：在 MMLU、MMLU-Pro 与 GPQA 等教育类基准上，DeepSeek-V3 超越了所有其他开源模型，分别取得了 88.5、75.9 和 59.1 的成绩，其表现可与 GPT-4o 和 Claude-Sonnet-3.5 等领先闭源模型相媲美，进一步缩小了开源与闭源模型在此领域的差距。
在客观事实相关基准上，DeepSeek-V3 在 SimpleQA 与 Chinese SimpleQA 两个数据集中均优于其他开源模型。尽管在英文事实类知识（SimpleQA）方面仍略逊于 GPT-4o 与 Claude-Sonnet-3.5，但在中文事实类知识（Chinese SimpleQA）上却超过了这些模型，展现出其在中文事实类知识方面的优势。

 - 代码、数学与推理：在数学相关基准上，DeepSeek-V3 在所有non-long-CoT的开源与闭源模型中取得了最先进的表现。值得注意的是，针对部分基准（如 MATH-500），其表现甚至超过了 o1-preview，展现了强大的数学推理能力。在编程相关任务上，DeepSeek-V3 在编程竞赛基准（如 LiveCodeBench）中取得最佳表现，牢牢占据该领域的领先地位。对于工程类任务，虽然 DeepSeek-V3 略低于 Claude-Sonnet-3.5，但依然远超其他所有模型，体现了其在多样化技术基准上的竞争力。

在后续的内容中，我们将首先详细介绍 DeepSeek-V3 的模型架构（第 2 节），包括高效推理所用的 Multi-head Latent Attention (MLA)（DeepSeek-AI, 2024c）以及经济训练所用的 DeepSeekMoE（Dai 等, 2024）等。随后，我们介绍所使用的基础设施，包括计算集群、训练框架、对 FP8 训练的支持、推理部署策略及未来硬件设计建议。接着，我们阐述预训练流程，包括训练数据的构建、超参数设置、长上下文扩展技术及相关评测与讨论（第 4 节）。然后，我们讨论后训练工作，包括有监督微调（SFT）、强化学习（RL）以及相应的评测与讨论（第 5 节）。最后，我们总结本项研究的结论，探讨 DeepSeek-V3 目前存在的限制并展望未来可能的研究方向（第 6 节）。



## 2.Architecture

我们首先介绍 DeepSeek-V3 的基本架构，其中采用了 Multi-head Latent Attention (MLA)（DeepSeek-AI, 2024c）来实现高效推理，并使用 DeepSeekMoE（Dai 等, 2024）来降低训练成本。随后，我们将介绍多 Token 预测（MTP）训练目标，该目标在评测基准上能够提升模型整体性能。除非另有说明，DeepSeek-V3 的其他细节均与 DeepSeek-V2（DeepSeek-AI, 2024c）中的设置保持一致。

### 2.1 基本架构

DeepSeek-V3 的整体框架仍然基于 Transformer（Vaswani 等, 2017）。为了实现高效推理与经济可控的训练成本，DeepSeek-V3 继续沿用 MLA 和 DeepSeekMoE，这两种方法在 DeepSeek-V2 中已得到充分验证。与 DeepSeek-V2 相比，主要的区别在于，我们在 DeepSeekMoE 中额外引入了一种无辅助损失（auxiliary-loss-free）的负载均衡策略（Wang 等, 2024a），以减轻因实现负载均衡而导致的性能下降。图 2 展示了 DeepSeek-V3 的基本架构，本节我们将简要回顾 MLA 与 DeepSeekMoE 的相关细节。

#### 2.1.1 Multi-Head Latent Attention





# 参考文献


